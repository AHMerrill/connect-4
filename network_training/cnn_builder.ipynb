{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# PERSISTENT SAVE SETUP (GOOGLE DRIVE)\n",
        "# ======================================================\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Base path inside Google Drive\n",
        "BASE_DIR = \"/content/drive/MyDrive/Colab Notebooks\"\n",
        "PROJECT_DIR = os.path.join(BASE_DIR, \"connect-4\")\n",
        "\n",
        "# Create project directory if it doesn't exist\n",
        "os.makedirs(PROJECT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Persistent save directory:\")\n",
        "print(f\"  {PROJECT_DIR}\")\n",
        "\n",
        "# Optional: subfolders for organization\n",
        "RESULTS_DIR = os.path.join(PROJECT_DIR, \"results\")\n",
        "MODELS_DIR  = os.path.join(PROJECT_DIR, \"models\")\n",
        "PLOTS_DIR   = os.path.join(PROJECT_DIR, \"plots\")\n",
        "\n",
        "for d in [RESULTS_DIR, MODELS_DIR, PLOTS_DIR]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "print(\"\\nSubdirectories:\")\n",
        "print(f\"  results → {RESULTS_DIR}\")\n",
        "print(f\"  models  → {MODELS_DIR}\")\n",
        "print(f\"  plots   → {PLOTS_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSZt3tYbhxaj",
        "outputId": "210e8edd-ac7b-4cab-c276-03e70108e5ad"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Persistent save directory:\n",
            "  /content/drive/MyDrive/Colab Notebooks/connect-4\n",
            "\n",
            "Subdirectories:\n",
            "  results → /content/drive/MyDrive/Colab Notebooks/connect-4/results\n",
            "  models  → /content/drive/MyDrive/Colab Notebooks/connect-4/models\n",
            "  plots   → /content/drive/MyDrive/Colab Notebooks/connect-4/plots\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connect-4 Policy/Value Network Training (MCTS-2000)\n",
        "\n",
        "This notebook is designed to run on **Google Colab**.\n",
        "\n",
        "Setup steps:\n",
        "1. Clone the `connect-4` GitHub repository into the Colab runtime\n",
        "2. Import shared utilities:\n",
        "   - `mirroring/mirror.py` for dataset symmetry handling\n",
        "   - `data_balance/balance.py` for move-depth weighting\n",
        "3. Download the official MCTS-2000 dataset from GitHub Releases\n",
        "\n",
        "No Google Drive mounting is required unless you want to persist trained models."
      ],
      "metadata": {
        "id": "zFMosyIoEn85"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jjUjcLZ10Cv",
        "outputId": "2fbcfc09-0b00-4263-a27e-a3e3983f9f95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.19.0\n",
            "GPUs available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "# ======================================================\n",
        "# COLAB SETUP — CLONE REPO & IMPORT UTILITIES\n",
        "# ======================================================\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pathlib import Path\n",
        "import urllib.request\n",
        "import tempfile\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# ------------------------------\n",
        "# 1. Clone the repo (once)\n",
        "# ------------------------------\n",
        "\n",
        "REPO_URL = \"https://github.com/AHMerrill/connect-4.git\"\n",
        "REPO_DIR = \"/content/connect-4\"\n",
        "\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    !git clone {REPO_URL} {REPO_DIR}\n",
        "\n",
        "# Make repo importable\n",
        "if REPO_DIR not in sys.path:\n",
        "    sys.path.append(REPO_DIR)\n",
        "\n",
        "# ------------------------------\n",
        "# 2. Import shared utilities\n",
        "# ------------------------------\n",
        "# mirror.py  -> connect-4/mirroring/mirror.py\n",
        "# balance.py -> connect-4/data_balance/balance.py\n",
        "\n",
        "from mirroring.mirror import mirror_dataset\n",
        "from data_balance.balance import compute_move_balance_weights\n",
        "\n",
        "# ------------------------------\n",
        "# 3. Reproducibility\n",
        "# ------------------------------\n",
        "\n",
        "SEED = 7\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPUs available:\", tf.config.list_physical_devices(\"GPU\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import mixed_precision\n",
        "\n",
        "mixed_precision.set_global_policy(\"mixed_float16\")"
      ],
      "metadata": {
        "id": "uQemJYGxjvpR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    tf.config.optimizer.set_jit(True)\n",
        "    print(\"XLA JIT enabled\")\n",
        "except Exception as e:\n",
        "    print(\"XLA JIT not available:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOmn2xZqnZPo",
        "outputId": "be3a8a41-a085-4ef0-fc44-ef31ce02927e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XLA JIT enabled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 — Load and Inspect the Raw Dataset (Dynamic)\n",
        "\n",
        "In this step we:\n",
        "\n",
        "- Download the Connect-4 MCTS dataset from its source (URL or local path)\n",
        "- Load the `.npz` archive **without assumptions** about its internal structure\n",
        "- Inspect available keys, shapes, and dtypes\n",
        "- Decide dynamically how to construct the dataset dictionary used downstream\n",
        "\n",
        "This cell does **no transformations** — it is purely for inspection and validation."
      ],
      "metadata": {
        "id": "Rj0Ifm4QFk5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# LOAD + INSPECT NPZ DATASET (NO ASSUMPTIONS)\n",
        "# ======================================================\n",
        "\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import urllib.request\n",
        "import tempfile\n",
        "\n",
        "DATASET_URL = (\n",
        "    \"https://github.com/AHMerrill/connect-4/\"\n",
        "    \"releases/download/v0.1-data/mcts_not_mirrored_2000.npz\"\n",
        ")\n",
        "\n",
        "def load_npz(source):\n",
        "    if str(source).startswith(\"http\"):\n",
        "        tmp_dir = Path(tempfile.gettempdir())\n",
        "        tmp_path = tmp_dir / Path(source).name\n",
        "        if not tmp_path.exists():\n",
        "            print(f\"Downloading dataset to {tmp_path} ...\")\n",
        "            urllib.request.urlretrieve(source, tmp_path)\n",
        "        return np.load(tmp_path, allow_pickle=True)\n",
        "    return np.load(Path(source), allow_pickle=True)\n",
        "\n",
        "# Load archive\n",
        "npz = load_npz(DATASET_URL)\n",
        "\n",
        "print(\"\\nNPZ file loaded.\")\n",
        "print(\"Available keys:\")\n",
        "for k in npz.files:\n",
        "    arr = npz[k]\n",
        "    if isinstance(arr, np.ndarray):\n",
        "        print(f\"  {k:10s} | shape={arr.shape} dtype={arr.dtype}\")\n",
        "    else:\n",
        "        print(f\"  {k:10s} | type={type(arr)}\")\n",
        "\n",
        "# Sanity checks\n",
        "required_keys = {\"X\", \"policy\", \"value\"}\n",
        "missing = required_keys - set(npz.files)\n",
        "if missing:\n",
        "    raise KeyError(f\"Missing required keys in NPZ: {missing}\")\n",
        "\n",
        "print(\"\\nBasic sanity checks:\")\n",
        "print(\"X sample shape:\", npz[\"X\"][0].shape)\n",
        "print(\"Policy sample:\", npz[\"policy\"][0])\n",
        "print(\"Value sample:\", npz[\"value\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koAJ1ByNFj7K",
        "outputId": "5297ead3-77ca-4841-ff14-9a86947d6b1a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NPZ file loaded.\n",
            "Available keys:\n",
            "  boards     | shape=(703111, 6, 7) dtype=int8\n",
            "  visits     | shape=(703111, 7) dtype=float32\n",
            "  scores     | shape=(703111, 7) dtype=float32\n",
            "  policy     | shape=(703111, 7) dtype=float32\n",
            "  q          | shape=(703111, 7) dtype=float32\n",
            "  value      | shape=(703111, 1) dtype=float32\n",
            "  X          | shape=(703111, 6, 7, 2) dtype=float32\n",
            "\n",
            "Basic sanity checks:\n",
            "X sample shape: (6, 7, 2)\n",
            "Policy sample: [0.1183     0.18783334 0.2223     0.06976666 0.2251     0.09663333\n",
            " 0.08006667]\n",
            "Value sample: [0.12936667]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3 — Mirror Boards and Apply Move-Depth Balancing (Dynamic)\n",
        "\n",
        "In this step we:\n",
        "\n",
        "- Construct the dataset dictionary **based on what actually exists** in the NPZ\n",
        "- Apply **board mirroring** using the shared utility\n",
        "- Apply **dynamic move-depth loss weighting**\n",
        "  - No samples dropped\n",
        "  - No resampling\n",
        "  - Weighting derived from the dataset’s true move distribution\n",
        "\n",
        "Outputs from this step are:\n",
        "- `mirrored_data` — ready for training\n",
        "- `sample_weights` — passed directly to `model.fit`"
      ],
      "metadata": {
        "id": "zYNUkx5kGXqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# MIRROR DATASET + COMPUTE MOVE-DEPTH WEIGHTS (FULLY DYNAMIC)\n",
        "# ======================================================\n",
        "\n",
        "from mirroring.mirror import mirror_dataset\n",
        "from data_balance.balance import compute_move_balance_weights\n",
        "import numpy as np\n",
        "\n",
        "# ------------------------------\n",
        "# 1. Build dataset dict dynamically\n",
        "# ------------------------------\n",
        "\n",
        "data = {\n",
        "    \"X\": npz[\"X\"],\n",
        "    \"policy\": npz[\"policy\"],\n",
        "    \"value\": npz[\"value\"],\n",
        "}\n",
        "\n",
        "# Optional fields (only include if present)\n",
        "for optional_key in [\"boards\", \"visits\", \"scores\", \"q\"]:\n",
        "    if optional_key in npz.files:\n",
        "        data[optional_key] = npz[optional_key]\n",
        "\n",
        "print(\"\\nDataset dictionary constructed with keys:\")\n",
        "for k, v in data.items():\n",
        "    print(f\"  {k:10s} | shape={v.shape}\")\n",
        "\n",
        "# ------------------------------\n",
        "# 2. Mirror dataset\n",
        "# ------------------------------\n",
        "\n",
        "mirrored_data = mirror_dataset(data)\n",
        "\n",
        "print(\"\\nAfter mirroring:\")\n",
        "for k, v in mirrored_data.items():\n",
        "    print(f\"  {k:10s} | shape={v.shape}\")\n",
        "\n",
        "# ------------------------------\n",
        "# 3. Compute move-depth balance weights\n",
        "# ------------------------------\n",
        "\n",
        "mirrored_data, sample_weights = compute_move_balance_weights(\n",
        "    mirrored_data,\n",
        "    num_bins=10,\n",
        ")\n",
        "\n",
        "print(\"\\nSample weights summary (to be used during training):\")\n",
        "print(f\"  shape : {sample_weights.shape}\")\n",
        "print(f\"  mean  : {sample_weights.mean():.4f}\")\n",
        "print(f\"  min   : {sample_weights.min():.4f}\")\n",
        "print(f\"  max   : {sample_weights.max():.4f}\")\n",
        "\n",
        "# NOTE:\n",
        "# `sample_weights` is a per-position loss multiplier.\n",
        "# It is NOT applied yet.\n",
        "# It will be passed into model.fit(...) later via `sample_weight=...`.\n",
        "\n",
        "# ------------------------------\n",
        "# 4. Fully data-driven dataset synopsis\n",
        "# ------------------------------\n",
        "\n",
        "X = mirrored_data[\"X\"]\n",
        "\n",
        "# Move count = number of stones on the board\n",
        "move_count = np.count_nonzero(\n",
        "    X[..., 0] + X[..., 1],\n",
        "    axis=(1, 2),\n",
        ")\n",
        "\n",
        "N = len(move_count)\n",
        "min_moves, max_moves = move_count.min(), move_count.max()\n",
        "\n",
        "# Quantile-based depth regions (no hard-coded assumptions)\n",
        "q_early, q_mid, q_late = np.quantile(move_count, [0.25, 0.50, 0.75])\n",
        "\n",
        "early_mask = move_count <= q_early\n",
        "mid_mask   = (move_count > q_early) & (move_count <= q_late)\n",
        "late_mask  = move_count > q_late\n",
        "\n",
        "early_frac = early_mask.mean()\n",
        "mid_frac   = mid_mask.mean()\n",
        "late_frac  = late_mask.mean()\n",
        "\n",
        "# Bin-level imbalance (same bins used for weighting)\n",
        "NUM_BINS = 10\n",
        "bins = np.linspace(min_moves, max_moves + 1, NUM_BINS + 1)\n",
        "bin_ids = np.digitize(move_count, bins) - 1\n",
        "bin_ids = np.clip(bin_ids, 0, NUM_BINS - 1)\n",
        "\n",
        "bin_counts = np.bincount(bin_ids, minlength=NUM_BINS)\n",
        "\n",
        "most_common = bin_counts.max()\n",
        "least_common = np.min(bin_counts[bin_counts > 0])\n",
        "imbalance_ratio = most_common / least_common\n",
        "\n",
        "dominant_bin = bin_counts.argmax()\n",
        "dominant_share = bin_counts[dominant_bin] / N\n",
        "\n",
        "print(\"\\nData-driven interpretation:\")\n",
        "print(\n",
        "    f\"  Positions span move counts from {min_moves} to {max_moves}.\\n\\n\"\n",
        "    f\"  Using empirical depth quantiles:\\n\"\n",
        "    f\"    • Early-game positions (≤ {int(q_early)} moves): {early_frac:.1%}\\n\"\n",
        "    f\"    • Mid-game positions ({int(q_early)+1}–{int(q_late)} moves): {mid_frac:.1%}\\n\"\n",
        "    f\"    • Late-game positions (> {int(q_late)} moves): {late_frac:.1%}\\n\\n\"\n",
        "    f\"  When grouped into {NUM_BINS} move-depth bins, the most populated bin contains \"\n",
        "    f\"{most_common:,} samples ({dominant_share:.1%} of the dataset), while the least \"\n",
        "    f\"populated non-empty bin contains {least_common:,} samples \"\n",
        "    f\"(≈ {imbalance_ratio:.1f}× fewer).\\n\\n\"\n",
        "    f\"  This means raw training would overweight positions from high-frequency depth \"\n",
        "    f\"regions and underweight rarer depths.\\n\\n\"\n",
        "    f\"  To correct for this, inverse-frequency loss weights are computed per sample \"\n",
        "    f\"based on its move-depth bin. These weights are stored in `sample_weights` and \"\n",
        "    f\"will be applied during training so that each depth region contributes more \"\n",
        "    f\"evenly in expectation.\\n\\n\"\n",
        "    f\"  No samples are dropped or duplicated. All statistics and weights are computed \"\n",
        "    f\"directly from the loaded dataset and will automatically adapt if a different \"\n",
        "    f\"dataset is used.\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jkz4Ps-hGZMn",
        "outputId": "735a5d3f-8ed7-4b01-c630-7ef1dfeb6a45"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset dictionary constructed with keys:\n",
            "  X          | shape=(703111, 6, 7, 2)\n",
            "  policy     | shape=(703111, 7)\n",
            "  value      | shape=(703111, 1)\n",
            "  boards     | shape=(703111, 6, 7)\n",
            "  visits     | shape=(703111, 7)\n",
            "  scores     | shape=(703111, 7)\n",
            "  q          | shape=(703111, 7)\n",
            "Mirroring complete: 703111 → 1406222 samples\n",
            "\n",
            "After mirroring:\n",
            "  X          | shape=(1406222, 6, 7, 2)\n",
            "  policy     | shape=(1406222, 7)\n",
            "  value      | shape=(1406222, 1)\n",
            "  boards     | shape=(1406222, 6, 7)\n",
            "  visits     | shape=(1406222, 7)\n",
            "  scores     | shape=(1406222, 7)\n",
            "  q          | shape=(1406222, 7)\n",
            "Move-count balancing:\n",
            "  samples           : 1406222\n",
            "  bins              : 10\n",
            "  move range        : [0, 41]\n",
            "  bin counts        : [2460, 145278, 373760, 329900, 231714, 171814, 78988, 45276, 20070, 6962]\n",
            "  bin weight range  : [1.000, 151.935]\n",
            "  sample weight μ   : 1.000\n",
            "\n",
            "Sample weights summary (to be used during training):\n",
            "  shape : (1406222,)\n",
            "  mean  : 1.0000\n",
            "  min   : 0.3762\n",
            "  max   : 57.1635\n",
            "\n",
            "Data-driven interpretation:\n",
            "  Positions span move counts from 0 to 41.\n",
            "\n",
            "  Using empirical depth quantiles:\n",
            "    • Early-game positions (≤ 11 moves): 30.3%\n",
            "    • Mid-game positions (12–20 moves): 46.7%\n",
            "    • Late-game positions (> 20 moves): 23.0%\n",
            "\n",
            "  When grouped into 10 move-depth bins, the most populated bin contains 373,760 samples (26.6% of the dataset), while the least populated non-empty bin contains 2,460 samples (≈ 151.9× fewer).\n",
            "\n",
            "  This means raw training would overweight positions from high-frequency depth regions and underweight rarer depths.\n",
            "\n",
            "  To correct for this, inverse-frequency loss weights are computed per sample based on its move-depth bin. These weights are stored in `sample_weights` and will be applied during training so that each depth region contributes more evenly in expectation.\n",
            "\n",
            "  No samples are dropped or duplicated. All statistics and weights are computed directly from the loaded dataset and will automatically adapt if a different dataset is used.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 — Stratified Train/Test Split by Move Depth (No Cross-Validation Yet)\n",
        "\n",
        "We will now create a **train/test split** that is **stratified by move depth** (number of stones on the board).\n",
        "\n",
        "Why:\n",
        "- The dataset is not uniformly distributed across move depths.\n",
        "- A purely random split can slightly shift the depth mix between train and test.\n",
        "- Stratification ensures **train and test have the same move-depth distribution**, making model comparisons fair.\n",
        "\n",
        "What this does **not** do:\n",
        "- No samples are dropped.\n",
        "- No resampling is performed.\n",
        "- No cross-validation is performed yet (we’ll consider it after initial results).\n",
        "\n",
        "We will still use the previously computed `sample_weights` during training to ensure\n",
        "**balanced gradient contribution** across move-depth bins."
      ],
      "metadata": {
        "id": "R_pSLeYWPQAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# STEP 4 — STRATIFIED TRAIN / TEST SPLIT BY MOVE DEPTH\n",
        "# (SPLIT *ALL* DATA FIELDS, FULLY ALIGNED)\n",
        "# ======================================================\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# Inputs (from previous steps):\n",
        "#   mirrored_data  : dict with X, policy, value, and optional fields (q, etc.)\n",
        "#   sample_weights : (N,) array from compute_move_balance_weights(...)\n",
        "# ------------------------------------------------------\n",
        "\n",
        "# ------------------------------\n",
        "# 0) Extract core arrays\n",
        "# ------------------------------\n",
        "\n",
        "X      = mirrored_data[\"X\"].astype(np.float32)\n",
        "policy = mirrored_data[\"policy\"].astype(np.float32)\n",
        "value  = mirrored_data[\"value\"].astype(np.float32)\n",
        "weights = sample_weights.astype(np.float32)\n",
        "\n",
        "# Optional fields (present in your dataset)\n",
        "Q = mirrored_data.get(\"q\", None)\n",
        "\n",
        "N = X.shape[0]\n",
        "print(f\"Total samples: {N:,}\")\n",
        "\n",
        "# ------------------------------\n",
        "# 1) Compute move count (stones on board)\n",
        "# ------------------------------\n",
        "\n",
        "move_count = np.count_nonzero(\n",
        "    X[..., 0] + X[..., 1],\n",
        "    axis=(1, 2),\n",
        ").astype(int)\n",
        "\n",
        "# ------------------------------\n",
        "# 2) Bin move counts for stratification\n",
        "# ------------------------------\n",
        "\n",
        "NUM_BINS = 10\n",
        "bins = np.linspace(move_count.min(), move_count.max() + 1, NUM_BINS + 1)\n",
        "\n",
        "bin_ids = np.digitize(move_count, bins) - 1\n",
        "bin_ids = np.clip(bin_ids, 0, NUM_BINS - 1)\n",
        "\n",
        "# ------------------------------\n",
        "# 3) Build split payload (THIS IS THE FIX)\n",
        "# ------------------------------\n",
        "\n",
        "split_payload = [\n",
        "    X,\n",
        "    policy,\n",
        "    value,\n",
        "    weights,\n",
        "    bin_ids,\n",
        "]\n",
        "\n",
        "payload_names = [\n",
        "    \"X\",\n",
        "    \"policy\",\n",
        "    \"value\",\n",
        "    \"weights\",\n",
        "    \"bin_ids\",\n",
        "]\n",
        "\n",
        "if Q is not None:\n",
        "    split_payload.append(Q.astype(np.float32))\n",
        "    payload_names.append(\"Q\")\n",
        "\n",
        "# ------------------------------\n",
        "# 4) Stratified train/test split (ALL FIELDS)\n",
        "# ------------------------------\n",
        "\n",
        "split = train_test_split(\n",
        "    *split_payload,\n",
        "    test_size=0.20,\n",
        "    random_state=SEED,\n",
        "    stratify=bin_ids,\n",
        ")\n",
        "\n",
        "# ------------------------------\n",
        "# 5) Unpack results cleanly\n",
        "# ------------------------------\n",
        "\n",
        "idx = 0\n",
        "X_train, X_test = split[idx], split[idx+1]; idx += 2\n",
        "policy_train, policy_test = split[idx], split[idx+1]; idx += 2\n",
        "value_train, value_test = split[idx], split[idx+1]; idx += 2\n",
        "w_train, w_test = split[idx], split[idx+1]; idx += 2\n",
        "bin_train, bin_test = split[idx], split[idx+1]; idx += 2\n",
        "\n",
        "if Q is not None:\n",
        "    Q_train, Q_test = split[idx], split[idx+1]\n",
        "\n",
        "# ------------------------------\n",
        "# 6) Diagnostics — verify stratification\n",
        "# ------------------------------\n",
        "\n",
        "def summarize_bins(name, b):\n",
        "    counts = np.bincount(b, minlength=NUM_BINS)\n",
        "    frac = counts / counts.sum()\n",
        "    print(f\"\\n{name} move-depth bin distribution:\")\n",
        "    for i in range(NUM_BINS):\n",
        "        print(f\"  bin {i:2d}: {counts[i]:8d} ({frac[i]:6.2%})\")\n",
        "\n",
        "print(\"\\nStratified split complete.\")\n",
        "summarize_bins(\"TRAIN\", bin_train)\n",
        "summarize_bins(\"TEST \", bin_test)\n",
        "\n",
        "print(\"\\nWeight sanity check (means should be ~1.0):\")\n",
        "print(f\"  mean(w_train) = {w_train.mean():.4f}\")\n",
        "print(f\"  mean(w_test)  = {w_test.mean():.4f}\")\n",
        "\n",
        "print(\"\\nShapes:\")\n",
        "print(f\"  X_train      : {X_train.shape}\")\n",
        "print(f\"  policy_train : {policy_train.shape}\")\n",
        "print(f\"  value_train  : {value_train.shape}\")\n",
        "print(f\"  X_test       : {X_test.shape}\")\n",
        "print(f\"  policy_test  : {policy_test.shape}\")\n",
        "print(f\"  value_test   : {value_test.shape}\")\n",
        "\n",
        "if Q is not None:\n",
        "    print(f\"  Q_train      : {Q_train.shape}\")\n",
        "    print(f\"  Q_test       : {Q_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TotKEiVCPTHE",
        "outputId": "5d98e898-0c3a-49e2-f506-ceb74b003f7d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 1,406,222\n",
            "\n",
            "Stratified split complete.\n",
            "\n",
            "TRAIN move-depth bin distribution:\n",
            "  bin  0:     1968 ( 0.17%)\n",
            "  bin  1:   116222 (10.33%)\n",
            "  bin  2:   299008 (26.58%)\n",
            "  bin  3:   263920 (23.46%)\n",
            "  bin  4:   185371 (16.48%)\n",
            "  bin  5:   137451 (12.22%)\n",
            "  bin  6:    63190 ( 5.62%)\n",
            "  bin  7:    36221 ( 3.22%)\n",
            "  bin  8:    16056 ( 1.43%)\n",
            "  bin  9:     5570 ( 0.50%)\n",
            "\n",
            "TEST  move-depth bin distribution:\n",
            "  bin  0:      492 ( 0.17%)\n",
            "  bin  1:    29056 (10.33%)\n",
            "  bin  2:    74752 (26.58%)\n",
            "  bin  3:    65980 (23.46%)\n",
            "  bin  4:    46343 (16.48%)\n",
            "  bin  5:    34363 (12.22%)\n",
            "  bin  6:    15798 ( 5.62%)\n",
            "  bin  7:     9055 ( 3.22%)\n",
            "  bin  8:     4014 ( 1.43%)\n",
            "  bin  9:     1392 ( 0.49%)\n",
            "\n",
            "Weight sanity check (means should be ~1.0):\n",
            "  mean(w_train) = 1.0000\n",
            "  mean(w_test)  = 1.0000\n",
            "\n",
            "Shapes:\n",
            "  X_train      : (1124977, 6, 7, 2)\n",
            "  policy_train : (1124977, 7)\n",
            "  value_train  : (1124977, 1)\n",
            "  X_test       : (281245, 6, 7, 2)\n",
            "  policy_test  : (281245, 7)\n",
            "  value_test   : (281245, 1)\n",
            "  Q_train      : (1124977, 7)\n",
            "  Q_test       : (281245, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5 — Define the CNN Model Grid (Residual Policy/Value Architectures)\n",
        "\n",
        "In this step we define a **small, deliberately constrained set of CNN architectures** to evaluate.\n",
        "\n",
        "The goal is to compare models in a way that is:\n",
        "- **Statistically clean**\n",
        "- **Interpretable**\n",
        "- **Focused on capacity where it actually matters** for Connect-4\n",
        "\n",
        "---\n",
        "\n",
        "### Design intent\n",
        "\n",
        "All models share the same **overall architecture and training setup**, differing only in a few carefully chosen capacity parameters.\n",
        "\n",
        "**Shared across all models**\n",
        "- AlphaZero-style **residual CNN** with separate policy and value heads\n",
        "- Input shape: `(6, 7, 2)` (board planes)\n",
        "- Optimizer: **AdamW**\n",
        "- Losses:\n",
        "  - Policy: categorical cross-entropy\n",
        "  - Value: mean squared error\n",
        "- Activations: ReLU\n",
        "- Move-depth **loss weighting** applied during training\n",
        "- Identical TRAIN / TEST split\n",
        "\n",
        "**Varied deliberately**\n",
        "- **Number of residual blocks** (network depth)\n",
        "- **Learning rate** (optimization dynamics)\n",
        "\n",
        "This keeps the comparison tight and ensures performance differences are driven by **model capacity and optimization behavior**, not confounded hyperparameter noise.\n",
        "\n",
        "---\n",
        "\n",
        "### What a residual block does\n",
        "\n",
        "Each residual block:\n",
        "- Applies two 3×3 convolutions with batch normalization\n",
        "- Adds the block’s input back to its output (skip connection)\n",
        "- Enables deeper networks without vanishing gradients\n",
        "\n",
        "Increasing the number of blocks increases **effective depth and representational power** while preserving stable optimization.\n",
        "\n",
        "---\n",
        "\n",
        "### What `filters` means\n",
        "\n",
        "The `filters` parameter controls the **channel width** of the network:\n",
        "- Each convolution produces `filters` feature maps\n",
        "- Higher values increase representational capacity at the cost of compute\n",
        "\n",
        "In this grid, all models use the **same number of filters**, so depth and learning rate are isolated as the primary variables.\n",
        "\n",
        "---\n",
        "\n",
        "### Policy and value head structure\n",
        "\n",
        "After the shared residual tower, the network splits into two heads:\n",
        "\n",
        "**Policy head**\n",
        "- 1×1 convolution → batch norm → ReLU\n",
        "- Flatten → Dense(7) with softmax\n",
        "- Outputs a probability distribution over the 7 columns\n",
        "\n",
        "**Value head**\n",
        "- 1×1 convolution → batch norm → ReLU\n",
        "- Flatten → Dense(64) → Dense(1) with tanh\n",
        "- Outputs a scalar value estimate in `[-1, 1]`\n",
        "\n",
        "Both heads are trained jointly using their respective losses.\n",
        "\n",
        "---\n",
        "\n",
        "### What this step does\n",
        "\n",
        "- Defines a reusable residual block\n",
        "- Defines a `build_residual_cnn()` factory function\n",
        "- Declares a compact model grid to be trained and evaluated in the next step"
      ],
      "metadata": {
        "id": "mL99s8XhWZKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# STEP 5 — DEFINE RESIDUAL CNN MODEL GRID\n",
        "# (MIXED PRECISION SAFE — FINAL)\n",
        "# ======================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# Residual block\n",
        "# ------------------------------------------------------\n",
        "\n",
        "def residual_block(x, filters):\n",
        "    \"\"\"\n",
        "    Standard AlphaZero-style residual block:\n",
        "      Conv → BN → ReLU → Conv → BN → Skip → ReLU\n",
        "    \"\"\"\n",
        "    skip = x\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(\n",
        "        filters,\n",
        "        kernel_size=3,\n",
        "        padding=\"same\",\n",
        "        use_bias=False,\n",
        "    )(x)\n",
        "    x = tf.keras.layers.BatchNormalization(dtype=\"float32\")(x)\n",
        "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(\n",
        "        filters,\n",
        "        kernel_size=3,\n",
        "        padding=\"same\",\n",
        "        use_bias=False,\n",
        "    )(x)\n",
        "    x = tf.keras.layers.BatchNormalization(dtype=\"float32\")(x)\n",
        "\n",
        "    x = tf.keras.layers.Add()([x, skip])\n",
        "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# Model builder\n",
        "# ------------------------------------------------------\n",
        "\n",
        "def build_residual_cnn(\n",
        "    input_shape=(6, 7, 2),\n",
        "    num_blocks=8,\n",
        "    filters=128,\n",
        "    learning_rate=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "):\n",
        "    \"\"\"\n",
        "    AlphaZero-style policy/value CNN.\n",
        "    Mixed precision safe:\n",
        "      - Internal layers run in float16\n",
        "      - BatchNorm + output heads run in float32\n",
        "    \"\"\"\n",
        "\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # Initial convolution\n",
        "    # --------------------------------------------------\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(\n",
        "        filters,\n",
        "        kernel_size=3,\n",
        "        padding=\"same\",\n",
        "        use_bias=False,\n",
        "    )(inputs)\n",
        "    x = tf.keras.layers.BatchNormalization(dtype=\"float32\")(x)\n",
        "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # Residual tower\n",
        "    # --------------------------------------------------\n",
        "\n",
        "    for _ in range(num_blocks):\n",
        "        x = residual_block(x, filters)\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # Policy head (FLOAT32 OUTPUT)\n",
        "    # --------------------------------------------------\n",
        "\n",
        "    p = tf.keras.layers.Conv2D(\n",
        "        2,\n",
        "        kernel_size=1,\n",
        "        use_bias=False,\n",
        "    )(x)\n",
        "    p = tf.keras.layers.BatchNormalization(dtype=\"float32\")(p)\n",
        "    p = tf.keras.layers.Activation(\"relu\")(p)\n",
        "    p = tf.keras.layers.Flatten()(p)\n",
        "\n",
        "    policy_out = tf.keras.layers.Dense(\n",
        "        7,\n",
        "        activation=\"softmax\",\n",
        "        dtype=\"float32\",   # CRITICAL FOR MIXED PRECISION\n",
        "        name=\"policy\",\n",
        "    )(p)\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # Value head (FLOAT32 OUTPUT)\n",
        "    # --------------------------------------------------\n",
        "\n",
        "    v = tf.keras.layers.Conv2D(\n",
        "        1,\n",
        "        kernel_size=1,\n",
        "        use_bias=False,\n",
        "    )(x)\n",
        "    v = tf.keras.layers.BatchNormalization(dtype=\"float32\")(v)\n",
        "    v = tf.keras.layers.Activation(\"relu\")(v)\n",
        "    v = tf.keras.layers.Flatten()(v)\n",
        "    v = tf.keras.layers.Dense(64, activation=\"relu\")(v)\n",
        "\n",
        "    value_out = tf.keras.layers.Dense(\n",
        "        1,\n",
        "        activation=\"tanh\",\n",
        "        dtype=\"float32\",   # CRITICAL FOR MIXED PRECISION\n",
        "        name=\"value\",\n",
        "    )(v)\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # Compile model\n",
        "    # --------------------------------------------------\n",
        "\n",
        "    model = tf.keras.Model(\n",
        "        inputs=inputs,\n",
        "        outputs=[policy_out, value_out],\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.AdamW(\n",
        "            learning_rate=learning_rate,\n",
        "            weight_decay=weight_decay,\n",
        "        ),\n",
        "        loss={\n",
        "            \"policy\": \"categorical_crossentropy\",\n",
        "            \"value\": \"mse\",\n",
        "        },\n",
        "        metrics={\n",
        "            \"policy\": [\"accuracy\"],\n",
        "            \"value\": [\"mse\"],\n",
        "        },\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# Model grid (FINAL — 4 MODELS)\n",
        "# ------------------------------------------------------\n",
        "\n",
        "MODEL_GRID = [\n",
        "    {\"num_blocks\": 8,  \"learning_rate\": 1e-3},\n",
        "    {\"num_blocks\": 10, \"learning_rate\": 1e-3},\n",
        "    {\"num_blocks\": 8,  \"learning_rate\": 3e-4},\n",
        "    {\"num_blocks\": 10, \"learning_rate\": 3e-4},\n",
        "]\n",
        "\n",
        "print(\"Model training plan:\")\n",
        "print(f\"  Total models to train: {len(MODEL_GRID)}\\n\")\n",
        "\n",
        "print(\"Fixed hyperparameters (shared):\")\n",
        "print(\"  • input_shape   : (6, 7, 2)\")\n",
        "print(\"  • filters       : 128\")\n",
        "print(\"  • weight_decay  : 1e-4\")\n",
        "print(\"  • optimizer     : AdamW\")\n",
        "print(\"  • loss_policy   : categorical_crossentropy\")\n",
        "print(\"  • loss_value    : mse\\n\")\n",
        "\n",
        "print(\"Model-specific configurations:\")\n",
        "for i, cfg in enumerate(MODEL_GRID, 1):\n",
        "    print(f\"\\n  Model {i}:\")\n",
        "    for k, v in cfg.items():\n",
        "        print(f\"    - {k:13s}: {v}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhQXsWrNWQM0",
        "outputId": "fae038ed-c0a9-4ceb-aaf8-f926135d66f4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training plan:\n",
            "  Total models to train: 4\n",
            "\n",
            "Fixed hyperparameters (shared):\n",
            "  • input_shape   : (6, 7, 2)\n",
            "  • filters       : 128\n",
            "  • weight_decay  : 1e-4\n",
            "  • optimizer     : AdamW\n",
            "  • loss_policy   : categorical_crossentropy\n",
            "  • loss_value    : mse\n",
            "\n",
            "Model-specific configurations:\n",
            "\n",
            "  Model 1:\n",
            "    - num_blocks   : 8\n",
            "    - learning_rate: 0.001\n",
            "\n",
            "  Model 2:\n",
            "    - num_blocks   : 10\n",
            "    - learning_rate: 0.001\n",
            "\n",
            "  Model 3:\n",
            "    - num_blocks   : 8\n",
            "    - learning_rate: 0.0003\n",
            "\n",
            "  Model 4:\n",
            "    - num_blocks   : 10\n",
            "    - learning_rate: 0.0003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6 — Train Model Grid with Early Stopping (TEST as Validation)\n",
        "\n",
        "In this step we train each candidate CNN architecture in the model grid and\n",
        "select the best-performing model **using the TEST split as a validation set**.\n",
        "\n",
        "This workflow is intentional and mirrors common **AlphaZero-style pipelines**\n",
        "where:\n",
        "- Offline data is used to compare architectures\n",
        "- Final evaluation happens through **actual gameplay**, not a static test set\n",
        "\n",
        "---\n",
        "\n",
        "### Key decisions (explicit and deliberate)\n",
        "\n",
        "- **TRAIN is used for gradient updates**\n",
        "- **TEST is used as validation**\n",
        "  - Monitors `val_loss` during training\n",
        "  - Drives early stopping\n",
        "  - Used to compare architectures and learning rates\n",
        "- **TEST is not considered a final benchmark**\n",
        "  - It is allowed to influence model selection\n",
        "  - It will be reincorporated later for final training\n",
        "- **True evaluation will be gameplay performance**\n",
        "  - Elo, win rate, or head-to-head matches vs baselines\n",
        "\n",
        "---\n",
        "\n",
        "### Early stopping behavior\n",
        "\n",
        "- Early stopping monitors **`val_loss` on TEST**\n",
        "- Training stops when TEST loss no longer improves\n",
        "- Best weights (lowest TEST loss) are restored automatically\n",
        "- This reveals:\n",
        "  - Effective capacity of each architecture\n",
        "  - Which models generalize better beyond TRAIN\n",
        "\n",
        "---\n",
        "\n",
        "### Loss weighting\n",
        "\n",
        "- Move-depth loss weights (`sample_weights`) are applied during training\n",
        "- This ensures:\n",
        "  - Balanced learning across early, mid, and late game positions\n",
        "  - No resampling or data duplication\n",
        "- The same weighting scheme is used consistently across all models\n",
        "\n",
        "---\n",
        "\n",
        "### What this step produces\n",
        "\n",
        "For each model in the grid, we record:\n",
        "- Training history (loss & accuracy curves)\n",
        "- Best validation (TEST) loss\n",
        "- Epoch where early stopping occurred\n",
        "- Training time\n",
        "\n",
        "These outputs are used to:\n",
        "- Select the **best architecture and learning rate**\n",
        "- Decide a **reasonable epoch count** for final training\n",
        "- Determine whether further tuning or cross-validation is warranted\n",
        "\n",
        "---\n",
        "\n",
        "### What this step does *not* do\n",
        "\n",
        "- Does **not** provide a final unbiased performance estimate\n",
        "- Does **not** evaluate gameplay strength\n",
        "- Does **not** claim generalization beyond this dataset\n",
        "\n",
        "All final claims about strength will come from **self-play or match results**,\n",
        "not this TEST split."
      ],
      "metadata": {
        "id": "6RKBi0WnWl4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# STEP 6 — TRAIN MODEL GRID WITH EARLY STOPPING\n",
        "# (TEST USED AS VALIDATION + PERSISTENT SAVES)\n",
        "# ======================================================\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# Training configuration\n",
        "# ------------------------------------------------------\n",
        "\n",
        "EPOCHS_MAX = 100\n",
        "BATCH_SIZE = 512\n",
        "PATIENCE = 10\n",
        "FILTERS = 128   # fixed across all models\n",
        "\n",
        "results = []            # summary per model\n",
        "histories = {}          # full training curves\n",
        "trained_models = {}     # trained model objects\n",
        "\n",
        "print(\"\\nSaving outputs to:\")\n",
        "print(f\"  RESULTS_DIR = {RESULTS_DIR}\")\n",
        "print(f\"  MODELS_DIR  = {MODELS_DIR}\")\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# Train each model\n",
        "# ------------------------------------------------------\n",
        "\n",
        "for i, cfg in enumerate(MODEL_GRID, start=1):\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"Training model {i}/{len(MODEL_GRID)} | config = {cfg}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    model = build_residual_cnn(\n",
        "        input_shape=(6, 7, 2),\n",
        "        num_blocks=cfg[\"num_blocks\"],\n",
        "        filters=FILTERS,\n",
        "        learning_rate=cfg[\"learning_rate\"],\n",
        "    )\n",
        "\n",
        "    # Early stopping based on VALIDATION (TEST) loss\n",
        "    early_stop = EarlyStopping(\n",
        "        monitor=\"val_loss\",\n",
        "        patience=PATIENCE,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1,\n",
        "    )\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train,\n",
        "        [policy_train, value_train],\n",
        "        sample_weight=[w_train, w_train],\n",
        "        validation_data=(\n",
        "            X_test,\n",
        "            [policy_test, value_test],\n",
        "            [w_test, w_test],\n",
        "        ),\n",
        "        epochs=EPOCHS_MAX,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=[early_stop],\n",
        "        verbose=2,\n",
        "        shuffle=True,\n",
        "    )\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # Record validation-driven results\n",
        "    # --------------------------------------------------\n",
        "\n",
        "    best_epoch = int(np.argmin(history.history[\"val_loss\"]) + 1)\n",
        "    best_val_loss = float(np.min(history.history[\"val_loss\"]))\n",
        "\n",
        "    results.append({\n",
        "        \"model_id\": i,\n",
        "        \"num_blocks\": cfg[\"num_blocks\"],\n",
        "        \"filters\": FILTERS,\n",
        "        \"learning_rate\": cfg[\"learning_rate\"],\n",
        "        \"best_epoch_val\": best_epoch,\n",
        "        \"best_val_loss\": best_val_loss,\n",
        "        \"train_time_sec\": elapsed,\n",
        "    })\n",
        "\n",
        "    histories[i] = history\n",
        "    trained_models[i] = model\n",
        "\n",
        "    print(\n",
        "        f\"\\nModel {i} summary:\"\n",
        "        f\"\\n  Residual blocks  : {cfg['num_blocks']}\"\n",
        "        f\"\\n  Filters          : {FILTERS}\"\n",
        "        f\"\\n  Learning rate    : {cfg['learning_rate']}\"\n",
        "        f\"\\n  Best epoch (val) : {best_epoch}\"\n",
        "        f\"\\n  Best val loss    : {best_val_loss:.4f}\"\n",
        "        f\"\\n  Train time (s)   : {elapsed:.1f}\"\n",
        "    )\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# Results table (sorted by validation loss)\n",
        "# ------------------------------------------------------\n",
        "\n",
        "results_df = (\n",
        "    pd.DataFrame(results)\n",
        "    .sort_values(\"best_val_loss\")\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "print(\"\\n=== MODEL COMPARISON (VALIDATION / TEST) ===\")\n",
        "display(results_df)\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# PERSIST RESULTS TO GOOGLE DRIVE\n",
        "# ------------------------------------------------------\n",
        "\n",
        "# 1) Save results table\n",
        "results_csv_path = os.path.join(RESULTS_DIR, \"model_grid_results.csv\")\n",
        "results_df.to_csv(results_csv_path, index=False)\n",
        "\n",
        "# 2) Save full training histories\n",
        "histories_path = os.path.join(RESULTS_DIR, \"training_histories.pkl\")\n",
        "with open(histories_path, \"wb\") as f:\n",
        "    pickle.dump(histories, f)\n",
        "\n",
        "# 3) Save best model (lowest validation loss)\n",
        "best_model_id = int(results_df.iloc[0][\"model_id\"])\n",
        "best_model_path = os.path.join(MODELS_DIR, \"best_model.keras\")\n",
        "trained_models[best_model_id].save(best_model_path)\n",
        "\n",
        "print(\"\\nSaved artifacts:\")\n",
        "print(f\"  Results table     → {results_csv_path}\")\n",
        "print(f\"  Training histories→ {histories_path}\")\n",
        "print(f\"  Best model        → {best_model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zCfcY_0FWlT_",
        "outputId": "e832b0fb-907c-4cc7-f1d7-1053b6a34079"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saving outputs to:\n",
            "  RESULTS_DIR = /content/drive/MyDrive/Colab Notebooks/connect-4/results\n",
            "  MODELS_DIR  = /content/drive/MyDrive/Colab Notebooks/connect-4/models\n",
            "\n",
            "======================================================================\n",
            "Training model 1/4 | config = {'num_blocks': 8, 'learning_rate': 0.001}\n",
            "======================================================================\n",
            "Epoch 1/100\n",
            "2198/2198 - 85s - 39ms/step - loss: 1.5890 - policy_accuracy: 0.5251 - policy_loss: 1.4008 - val_loss: 1.3949 - val_policy_accuracy: 0.6444 - val_policy_loss: 1.2677 - val_value_loss: 0.1270 - val_value_mse: 0.0658 - value_loss: 0.1884 - value_mse: 0.1029\n",
            "Epoch 2/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.3372 - policy_accuracy: 0.6673 - policy_loss: 1.2377 - val_loss: 1.3464 - val_policy_accuracy: 0.6690 - val_policy_loss: 1.2368 - val_value_loss: 0.1094 - val_value_mse: 0.0696 - value_loss: 0.0998 - value_mse: 0.0478\n",
            "Epoch 3/100\n",
            "2198/2198 - 17s - 8ms/step - loss: 1.2655 - policy_accuracy: 0.7010 - policy_loss: 1.2055 - val_loss: 1.2681 - val_policy_accuracy: 0.7113 - val_policy_loss: 1.2026 - val_value_loss: 0.0652 - val_value_mse: 0.0434 - value_loss: 0.0604 - value_mse: 0.0325\n",
            "Epoch 4/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.2270 - policy_accuracy: 0.7181 - policy_loss: 1.1896 - val_loss: 1.2544 - val_policy_accuracy: 0.7212 - val_policy_loss: 1.1934 - val_value_loss: 0.0608 - val_value_mse: 0.0545 - value_loss: 0.0377 - value_mse: 0.0233\n",
            "Epoch 5/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.2067 - policy_accuracy: 0.7297 - policy_loss: 1.1800 - val_loss: 1.2201 - val_policy_accuracy: 0.7338 - val_policy_loss: 1.1862 - val_value_loss: 0.0337 - val_value_mse: 0.0248 - value_loss: 0.0270 - value_mse: 0.0185\n",
            "Epoch 6/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1931 - policy_accuracy: 0.7391 - policy_loss: 1.1729 - val_loss: 1.2174 - val_policy_accuracy: 0.7353 - val_policy_loss: 1.1846 - val_value_loss: 0.0326 - val_value_mse: 0.0202 - value_loss: 0.0205 - value_mse: 0.0152\n",
            "Epoch 7/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1846 - policy_accuracy: 0.7461 - policy_loss: 1.1679 - val_loss: 1.1991 - val_policy_accuracy: 0.7443 - val_policy_loss: 1.1767 - val_value_loss: 0.0222 - val_value_mse: 0.0153 - value_loss: 0.0170 - value_mse: 0.0133\n",
            "Epoch 8/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1788 - policy_accuracy: 0.7508 - policy_loss: 1.1643 - val_loss: 1.1906 - val_policy_accuracy: 0.7506 - val_policy_loss: 1.1739 - val_value_loss: 0.0166 - val_value_mse: 0.0122 - value_loss: 0.0148 - value_mse: 0.0120\n",
            "Epoch 9/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1752 - policy_accuracy: 0.7548 - policy_loss: 1.1614 - val_loss: 1.1874 - val_policy_accuracy: 0.7552 - val_policy_loss: 1.1721 - val_value_loss: 0.0151 - val_value_mse: 0.0107 - value_loss: 0.0140 - value_mse: 0.0111\n",
            "Epoch 10/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1670 - policy_accuracy: 0.7614 - policy_loss: 1.1566 - val_loss: 1.1972 - val_policy_accuracy: 0.7549 - val_policy_loss: 1.1765 - val_value_loss: 0.0205 - val_value_mse: 0.0128 - value_loss: 0.0107 - value_mse: 0.0095\n",
            "Epoch 11/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1656 - policy_accuracy: 0.7626 - policy_loss: 1.1553 - val_loss: 1.1883 - val_policy_accuracy: 0.7583 - val_policy_loss: 1.1735 - val_value_loss: 0.0145 - val_value_mse: 0.0101 - value_loss: 0.0107 - value_mse: 0.0094\n",
            "Epoch 12/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1627 - policy_accuracy: 0.7655 - policy_loss: 1.1531 - val_loss: 1.1849 - val_policy_accuracy: 0.7631 - val_policy_loss: 1.1713 - val_value_loss: 0.0134 - val_value_mse: 0.0096 - value_loss: 0.0099 - value_mse: 0.0088\n",
            "Epoch 13/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1566 - policy_accuracy: 0.7704 - policy_loss: 1.1490 - val_loss: 1.2094 - val_policy_accuracy: 0.7446 - val_policy_loss: 1.1882 - val_value_loss: 0.0210 - val_value_mse: 0.0141 - value_loss: 0.0079 - value_mse: 0.0076\n",
            "Epoch 14/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1586 - policy_accuracy: 0.7699 - policy_loss: 1.1495 - val_loss: 1.1883 - val_policy_accuracy: 0.7625 - val_policy_loss: 1.1741 - val_value_loss: 0.0140 - val_value_mse: 0.0093 - value_loss: 0.0094 - value_mse: 0.0084\n",
            "Epoch 15/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1541 - policy_accuracy: 0.7735 - policy_loss: 1.1465 - val_loss: 1.1854 - val_policy_accuracy: 0.7654 - val_policy_loss: 1.1725 - val_value_loss: 0.0127 - val_value_mse: 0.0089 - value_loss: 0.0079 - value_mse: 0.0074\n",
            "Epoch 16/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1549 - policy_accuracy: 0.7737 - policy_loss: 1.1464 - val_loss: 1.1975 - val_policy_accuracy: 0.7601 - val_policy_loss: 1.1805 - val_value_loss: 0.0169 - val_value_mse: 0.0112 - value_loss: 0.0088 - value_mse: 0.0078\n",
            "Epoch 17/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1492 - policy_accuracy: 0.7785 - policy_loss: 1.1433 - val_loss: 1.1871 - val_policy_accuracy: 0.7678 - val_policy_loss: 1.1756 - val_value_loss: 0.0114 - val_value_mse: 0.0084 - value_loss: 0.0061 - value_mse: 0.0063\n",
            "Epoch 18/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1478 - policy_accuracy: 0.7800 - policy_loss: 1.1418 - val_loss: 1.1880 - val_policy_accuracy: 0.7703 - val_policy_loss: 1.1772 - val_value_loss: 0.0107 - val_value_mse: 0.0078 - value_loss: 0.0063 - value_mse: 0.0063\n",
            "Epoch 19/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1475 - policy_accuracy: 0.7806 - policy_loss: 1.1414 - val_loss: 1.2004 - val_policy_accuracy: 0.7621 - val_policy_loss: 1.1834 - val_value_loss: 0.0169 - val_value_mse: 0.0128 - value_loss: 0.0064 - value_mse: 0.0063\n",
            "Epoch 20/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1451 - policy_accuracy: 0.7830 - policy_loss: 1.1397 - val_loss: 1.1898 - val_policy_accuracy: 0.7666 - val_policy_loss: 1.1761 - val_value_loss: 0.0135 - val_value_mse: 0.0103 - value_loss: 0.0057 - value_mse: 0.0059\n",
            "Epoch 21/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1443 - policy_accuracy: 0.7840 - policy_loss: 1.1395 - val_loss: 1.1838 - val_policy_accuracy: 0.7729 - val_policy_loss: 1.1740 - val_value_loss: 0.0096 - val_value_mse: 0.0071 - value_loss: 0.0051 - value_mse: 0.0056\n",
            "Epoch 22/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1448 - policy_accuracy: 0.7847 - policy_loss: 1.1392 - val_loss: 1.1902 - val_policy_accuracy: 0.7694 - val_policy_loss: 1.1777 - val_value_loss: 0.0123 - val_value_mse: 0.0085 - value_loss: 0.0060 - value_mse: 0.0060\n",
            "Epoch 23/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1427 - policy_accuracy: 0.7869 - policy_loss: 1.1378 - val_loss: 1.1843 - val_policy_accuracy: 0.7727 - val_policy_loss: 1.1742 - val_value_loss: 0.0100 - val_value_mse: 0.0072 - value_loss: 0.0052 - value_mse: 0.0055\n",
            "Epoch 24/100\n",
            "2198/2198 - 17s - 8ms/step - loss: 1.1386 - policy_accuracy: 0.7906 - policy_loss: 1.1351 - val_loss: 1.1866 - val_policy_accuracy: 0.7742 - val_policy_loss: 1.1769 - val_value_loss: 0.0095 - val_value_mse: 0.0071 - value_loss: 0.0039 - value_mse: 0.0047\n",
            "Epoch 25/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1413 - policy_accuracy: 0.7889 - policy_loss: 1.1364 - val_loss: 1.1882 - val_policy_accuracy: 0.7711 - val_policy_loss: 1.1757 - val_value_loss: 0.0124 - val_value_mse: 0.0088 - value_loss: 0.0052 - value_mse: 0.0055\n",
            "Epoch 26/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1403 - policy_accuracy: 0.7904 - policy_loss: 1.1355 - val_loss: 1.1918 - val_policy_accuracy: 0.7758 - val_policy_loss: 1.1814 - val_value_loss: 0.0103 - val_value_mse: 0.0072 - value_loss: 0.0051 - value_mse: 0.0053\n",
            "Epoch 27/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1386 - policy_accuracy: 0.7924 - policy_loss: 1.1340 - val_loss: 1.2086 - val_policy_accuracy: 0.7655 - val_policy_loss: 1.1869 - val_value_loss: 0.0216 - val_value_mse: 0.0114 - value_loss: 0.0049 - value_mse: 0.0051\n",
            "Epoch 28/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1398 - policy_accuracy: 0.7913 - policy_loss: 1.1351 - val_loss: 1.1953 - val_policy_accuracy: 0.7725 - val_policy_loss: 1.1812 - val_value_loss: 0.0139 - val_value_mse: 0.0087 - value_loss: 0.0050 - value_mse: 0.0051\n",
            "Epoch 29/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1366 - policy_accuracy: 0.7946 - policy_loss: 1.1329 - val_loss: 1.1837 - val_policy_accuracy: 0.7761 - val_policy_loss: 1.1741 - val_value_loss: 0.0095 - val_value_mse: 0.0070 - value_loss: 0.0040 - value_mse: 0.0046\n",
            "Epoch 30/100\n",
            "2198/2198 - 17s - 8ms/step - loss: 1.1341 - policy_accuracy: 0.7976 - policy_loss: 1.1310 - val_loss: 1.1925 - val_policy_accuracy: 0.7760 - val_policy_loss: 1.1824 - val_value_loss: 0.0100 - val_value_mse: 0.0083 - value_loss: 0.0034 - value_mse: 0.0041\n",
            "Epoch 31/100\n",
            "2198/2198 - 17s - 8ms/step - loss: 1.1397 - policy_accuracy: 0.7925 - policy_loss: 1.1346 - val_loss: 1.1978 - val_policy_accuracy: 0.7741 - val_policy_loss: 1.1868 - val_value_loss: 0.0108 - val_value_mse: 0.0081 - value_loss: 0.0054 - value_mse: 0.0053\n",
            "Epoch 32/100\n",
            "2198/2198 - 17s - 8ms/step - loss: 1.1331 - policy_accuracy: 0.7994 - policy_loss: 1.1302 - val_loss: 1.1833 - val_policy_accuracy: 0.7766 - val_policy_loss: 1.1746 - val_value_loss: 0.0085 - val_value_mse: 0.0064 - value_loss: 0.0032 - value_mse: 0.0040\n",
            "Epoch 33/100\n",
            "2198/2198 - 17s - 8ms/step - loss: 1.1357 - policy_accuracy: 0.7971 - policy_loss: 1.1321 - val_loss: 1.1918 - val_policy_accuracy: 0.7771 - val_policy_loss: 1.1827 - val_value_loss: 0.0090 - val_value_mse: 0.0067 - value_loss: 0.0039 - value_mse: 0.0044\n",
            "Epoch 34/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1343 - policy_accuracy: 0.7985 - policy_loss: 1.1309 - val_loss: 1.1928 - val_policy_accuracy: 0.7778 - val_policy_loss: 1.1837 - val_value_loss: 0.0089 - val_value_mse: 0.0066 - value_loss: 0.0038 - value_mse: 0.0043\n",
            "Epoch 35/100\n",
            "2198/2198 - 17s - 8ms/step - loss: 1.1331 - policy_accuracy: 0.8002 - policy_loss: 1.1299 - val_loss: 1.1907 - val_policy_accuracy: 0.7753 - val_policy_loss: 1.1802 - val_value_loss: 0.0104 - val_value_mse: 0.0073 - value_loss: 0.0034 - value_mse: 0.0041\n",
            "Epoch 36/100\n",
            "2198/2198 - 17s - 8ms/step - loss: 1.1327 - policy_accuracy: 0.8007 - policy_loss: 1.1298 - val_loss: 1.1928 - val_policy_accuracy: 0.7771 - val_policy_loss: 1.1837 - val_value_loss: 0.0090 - val_value_mse: 0.0066 - value_loss: 0.0032 - value_mse: 0.0040\n",
            "Epoch 37/100\n",
            "2198/2198 - 17s - 8ms/step - loss: 1.1345 - policy_accuracy: 0.8000 - policy_loss: 1.1307 - val_loss: 1.1879 - val_policy_accuracy: 0.7766 - val_policy_loss: 1.1784 - val_value_loss: 0.0093 - val_value_mse: 0.0070 - value_loss: 0.0042 - value_mse: 0.0043\n",
            "Epoch 38/100\n",
            "2198/2198 - 17s - 8ms/step - loss: 1.1332 - policy_accuracy: 0.8015 - policy_loss: 1.1298 - val_loss: 1.1957 - val_policy_accuracy: 0.7787 - val_policy_loss: 1.1865 - val_value_loss: 0.0091 - val_value_mse: 0.0064 - value_loss: 0.0037 - value_mse: 0.0042\n",
            "Epoch 39/100\n",
            "2198/2198 - 17s - 8ms/step - loss: 1.1304 - policy_accuracy: 0.8046 - policy_loss: 1.1277 - val_loss: 1.1926 - val_policy_accuracy: 0.7805 - val_policy_loss: 1.1844 - val_value_loss: 0.0081 - val_value_mse: 0.0061 - value_loss: 0.0030 - value_mse: 0.0037\n",
            "Epoch 40/100\n",
            "2198/2198 - 17s - 8ms/step - loss: 1.1362 - policy_accuracy: 0.7986 - policy_loss: 1.1318 - val_loss: 1.1985 - val_policy_accuracy: 0.7723 - val_policy_loss: 1.1852 - val_value_loss: 0.0132 - val_value_mse: 0.0095 - value_loss: 0.0048 - value_mse: 0.0048\n",
            "Epoch 41/100\n",
            "2198/2198 - 17s - 8ms/step - loss: 1.1310 - policy_accuracy: 0.8044 - policy_loss: 1.1282 - val_loss: 1.1955 - val_policy_accuracy: 0.7809 - val_policy_loss: 1.1873 - val_value_loss: 0.0081 - val_value_mse: 0.0059 - value_loss: 0.0032 - value_mse: 0.0038\n",
            "Epoch 42/100\n",
            "2198/2198 - 17s - 8ms/step - loss: 1.1289 - policy_accuracy: 0.8073 - policy_loss: 1.1266 - val_loss: 1.2037 - val_policy_accuracy: 0.7678 - val_policy_loss: 1.1883 - val_value_loss: 0.0153 - val_value_mse: 0.0102 - value_loss: 0.0026 - value_mse: 0.0034\n",
            "Epoch 42: early stopping\n",
            "Restoring model weights from the end of the best epoch: 32.\n",
            "\n",
            "Model 1 summary:\n",
            "  Residual blocks  : 8\n",
            "  Filters          : 128\n",
            "  Learning rate    : 0.001\n",
            "  Best epoch (val) : 32\n",
            "  Best val loss    : 1.1833\n",
            "  Train time (s)   : 804.5\n",
            "\n",
            "======================================================================\n",
            "Training model 2/4 | config = {'num_blocks': 10, 'learning_rate': 0.001}\n",
            "======================================================================\n",
            "Epoch 1/100\n",
            "2198/2198 - 55s - 25ms/step - loss: 1.5619 - policy_accuracy: 0.5405 - policy_loss: 1.3825 - val_loss: 1.3773 - val_policy_accuracy: 0.6461 - val_policy_loss: 1.2577 - val_value_loss: 0.1193 - val_value_mse: 0.0575 - value_loss: 0.1796 - value_mse: 0.0964\n",
            "Epoch 2/100\n",
            "2198/2198 - 21s - 9ms/step - loss: 1.3216 - policy_accuracy: 0.6752 - policy_loss: 1.2298 - val_loss: 1.3066 - val_policy_accuracy: 0.6846 - val_policy_loss: 1.2265 - val_value_loss: 0.0799 - val_value_mse: 0.0415 - value_loss: 0.0921 - value_mse: 0.0451\n",
            "Epoch 3/100\n",
            "2198/2198 - 21s - 9ms/step - loss: 1.2560 - policy_accuracy: 0.7049 - policy_loss: 1.2021 - val_loss: 1.2413 - val_policy_accuracy: 0.7204 - val_policy_loss: 1.1941 - val_value_loss: 0.0470 - val_value_mse: 0.0269 - value_loss: 0.0541 - value_mse: 0.0304\n",
            "Epoch 4/100\n",
            "2198/2198 - 21s - 9ms/step - loss: 1.2173 - policy_accuracy: 0.7251 - policy_loss: 1.1850 - val_loss: 1.2229 - val_policy_accuracy: 0.7293 - val_policy_loss: 1.1877 - val_value_loss: 0.0350 - val_value_mse: 0.0213 - value_loss: 0.0326 - value_mse: 0.0208\n",
            "Epoch 5/100\n",
            "2198/2198 - 21s - 9ms/step - loss: 1.2038 - policy_accuracy: 0.7324 - policy_loss: 1.1787 - val_loss: 1.2461 - val_policy_accuracy: 0.7112 - val_policy_loss: 1.2053 - val_value_loss: 0.0406 - val_value_mse: 0.0267 - value_loss: 0.0254 - value_mse: 0.0176\n",
            "Epoch 6/100\n",
            "2198/2198 - 21s - 9ms/step - loss: 1.1925 - policy_accuracy: 0.7418 - policy_loss: 1.1725 - val_loss: 1.2840 - val_policy_accuracy: 0.7068 - val_policy_loss: 1.2058 - val_value_loss: 0.0780 - val_value_mse: 0.0623 - value_loss: 0.0204 - value_mse: 0.0148\n",
            "Epoch 7/100\n",
            "2198/2198 - 21s - 9ms/step - loss: 1.1833 - policy_accuracy: 0.7493 - policy_loss: 1.1671 - val_loss: 1.1946 - val_policy_accuracy: 0.7475 - val_policy_loss: 1.1749 - val_value_loss: 0.0195 - val_value_mse: 0.0139 - value_loss: 0.0165 - value_mse: 0.0128\n",
            "Epoch 8/100\n",
            "2198/2198 - 21s - 10ms/step - loss: 1.1750 - policy_accuracy: 0.7558 - policy_loss: 1.1623 - val_loss: 1.1922 - val_policy_accuracy: 0.7527 - val_policy_loss: 1.1731 - val_value_loss: 0.0189 - val_value_mse: 0.0126 - value_loss: 0.0130 - value_mse: 0.0108\n",
            "Epoch 9/100\n",
            "2198/2198 - 21s - 9ms/step - loss: 1.1722 - policy_accuracy: 0.7584 - policy_loss: 1.1599 - val_loss: 1.1921 - val_policy_accuracy: 0.7513 - val_policy_loss: 1.1738 - val_value_loss: 0.0181 - val_value_mse: 0.0135 - value_loss: 0.0126 - value_mse: 0.0104\n",
            "Epoch 10/100\n",
            "2198/2198 - 21s - 10ms/step - loss: 1.1674 - policy_accuracy: 0.7625 - policy_loss: 1.1568 - val_loss: 1.1882 - val_policy_accuracy: 0.7584 - val_policy_loss: 1.1732 - val_value_loss: 0.0148 - val_value_mse: 0.0106 - value_loss: 0.0109 - value_mse: 0.0094\n",
            "Epoch 11/100\n",
            "2198/2198 - 21s - 9ms/step - loss: 1.1643 - policy_accuracy: 0.7652 - policy_loss: 1.1546 - val_loss: 1.1840 - val_policy_accuracy: 0.7601 - val_policy_loss: 1.1702 - val_value_loss: 0.0137 - val_value_mse: 0.0095 - value_loss: 0.0100 - value_mse: 0.0088\n",
            "Epoch 12/100\n",
            "2198/2198 - 21s - 9ms/step - loss: 1.1589 - policy_accuracy: 0.7701 - policy_loss: 1.1509 - val_loss: 1.1818 - val_policy_accuracy: 0.7653 - val_policy_loss: 1.1705 - val_value_loss: 0.0112 - val_value_mse: 0.0083 - value_loss: 0.0084 - value_mse: 0.0077\n",
            "Epoch 13/100\n",
            "2198/2198 - 21s - 9ms/step - loss: 1.1586 - policy_accuracy: 0.7706 - policy_loss: 1.1503 - val_loss: 1.1867 - val_policy_accuracy: 0.7651 - val_policy_loss: 1.1719 - val_value_loss: 0.0146 - val_value_mse: 0.0112 - value_loss: 0.0086 - value_mse: 0.0079\n",
            "Epoch 14/100\n",
            "2198/2198 - 21s - 9ms/step - loss: 1.1535 - policy_accuracy: 0.7753 - policy_loss: 1.1466 - val_loss: 1.1901 - val_policy_accuracy: 0.7638 - val_policy_loss: 1.1748 - val_value_loss: 0.0152 - val_value_mse: 0.0104 - value_loss: 0.0072 - value_mse: 0.0069\n",
            "Epoch 15/100\n",
            "2198/2198 - 21s - 9ms/step - loss: 1.1501 - policy_accuracy: 0.7785 - policy_loss: 1.1442 - val_loss: 1.1861 - val_policy_accuracy: 0.7682 - val_policy_loss: 1.1745 - val_value_loss: 0.0116 - val_value_mse: 0.0083 - value_loss: 0.0063 - value_mse: 0.0063\n",
            "Epoch 16/100\n",
            "2198/2198 - 21s - 9ms/step - loss: 1.1523 - policy_accuracy: 0.7772 - policy_loss: 1.1452 - val_loss: 1.1883 - val_policy_accuracy: 0.7656 - val_policy_loss: 1.1746 - val_value_loss: 0.0136 - val_value_mse: 0.0098 - value_loss: 0.0074 - value_mse: 0.0069\n",
            "Epoch 17/100\n",
            "2198/2198 - 21s - 9ms/step - loss: 1.1463 - policy_accuracy: 0.7825 - policy_loss: 1.1414 - val_loss: 1.1830 - val_policy_accuracy: 0.7684 - val_policy_loss: 1.1716 - val_value_loss: 0.0113 - val_value_mse: 0.0083 - value_loss: 0.0052 - value_mse: 0.0057\n",
            "Epoch 18/100\n",
            "2198/2198 - 21s - 9ms/step - loss: 1.1493 - policy_accuracy: 0.7806 - policy_loss: 1.1427 - val_loss: 1.1846 - val_policy_accuracy: 0.7683 - val_policy_loss: 1.1724 - val_value_loss: 0.0121 - val_value_mse: 0.0088 - value_loss: 0.0069 - value_mse: 0.0065\n",
            "Epoch 19/100\n",
            "2198/2198 - 21s - 9ms/step - loss: 1.1461 - policy_accuracy: 0.7838 - policy_loss: 1.1408 - val_loss: 1.1834 - val_policy_accuracy: 0.7736 - val_policy_loss: 1.1721 - val_value_loss: 0.0111 - val_value_mse: 0.0082 - value_loss: 0.0056 - value_mse: 0.0058\n",
            "Epoch 20/100\n",
            "2198/2198 - 21s - 9ms/step - loss: 1.1421 - policy_accuracy: 0.7876 - policy_loss: 1.1378 - val_loss: 1.1852 - val_policy_accuracy: 0.7729 - val_policy_loss: 1.1741 - val_value_loss: 0.0110 - val_value_mse: 0.0076 - value_loss: 0.0046 - value_mse: 0.0051\n",
            "Epoch 21/100\n",
            "2198/2198 - 21s - 9ms/step - loss: 1.1460 - policy_accuracy: 0.7852 - policy_loss: 1.1400 - val_loss: 1.1834 - val_policy_accuracy: 0.7713 - val_policy_loss: 1.1716 - val_value_loss: 0.0116 - val_value_mse: 0.0083 - value_loss: 0.0063 - value_mse: 0.0060\n",
            "Epoch 22/100\n",
            "2198/2198 - 21s - 9ms/step - loss: 1.1405 - policy_accuracy: 0.7906 - policy_loss: 1.1365 - val_loss: 1.1856 - val_policy_accuracy: 0.7755 - val_policy_loss: 1.1749 - val_value_loss: 0.0106 - val_value_mse: 0.0076 - value_loss: 0.0043 - value_mse: 0.0049\n",
            "Epoch 22: early stopping\n",
            "Restoring model weights from the end of the best epoch: 12.\n",
            "\n",
            "Model 2 summary:\n",
            "  Residual blocks  : 10\n",
            "  Filters          : 128\n",
            "  Learning rate    : 0.001\n",
            "  Best epoch (val) : 12\n",
            "  Best val loss    : 1.1818\n",
            "  Train time (s)   : 493.1\n",
            "\n",
            "======================================================================\n",
            "Training model 3/4 | config = {'num_blocks': 8, 'learning_rate': 0.0003}\n",
            "======================================================================\n",
            "Epoch 1/100\n",
            "2198/2198 - 47s - 22ms/step - loss: 1.6035 - policy_accuracy: 0.5152 - policy_loss: 1.4104 - val_loss: 1.4643 - val_policy_accuracy: 0.5905 - val_policy_loss: 1.3108 - val_value_loss: 0.1532 - val_value_mse: 0.0782 - value_loss: 0.1934 - value_mse: 0.1047\n",
            "Epoch 2/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.3744 - policy_accuracy: 0.6458 - policy_loss: 1.2608 - val_loss: 1.3479 - val_policy_accuracy: 0.6569 - val_policy_loss: 1.2509 - val_value_loss: 0.0967 - val_value_mse: 0.0481 - value_loss: 0.1139 - value_mse: 0.0548\n",
            "Epoch 3/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.3019 - policy_accuracy: 0.6823 - policy_loss: 1.2224 - val_loss: 1.3098 - val_policy_accuracy: 0.6857 - val_policy_loss: 1.2263 - val_value_loss: 0.0832 - val_value_mse: 0.0476 - value_loss: 0.0797 - value_mse: 0.0407\n",
            "Epoch 4/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.2592 - policy_accuracy: 0.7008 - policy_loss: 1.2033 - val_loss: 1.2935 - val_policy_accuracy: 0.6984 - val_policy_loss: 1.2156 - val_value_loss: 0.0777 - val_value_mse: 0.0352 - value_loss: 0.0563 - value_mse: 0.0315\n",
            "Epoch 5/100\n",
            "2198/2198 - 19s - 8ms/step - loss: 1.2334 - policy_accuracy: 0.7126 - policy_loss: 1.1919 - val_loss: 1.2658 - val_policy_accuracy: 0.7089 - val_policy_loss: 1.2052 - val_value_loss: 0.0604 - val_value_mse: 0.0308 - value_loss: 0.0418 - value_mse: 0.0255\n",
            "Epoch 6/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.2132 - policy_accuracy: 0.7233 - policy_loss: 1.1826 - val_loss: 1.2386 - val_policy_accuracy: 0.7204 - val_policy_loss: 1.1964 - val_value_loss: 0.0420 - val_value_mse: 0.0232 - value_loss: 0.0309 - value_mse: 0.0206\n",
            "Epoch 7/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.2017 - policy_accuracy: 0.7296 - policy_loss: 1.1769 - val_loss: 1.2299 - val_policy_accuracy: 0.7226 - val_policy_loss: 1.1935 - val_value_loss: 0.0362 - val_value_mse: 0.0204 - value_loss: 0.0251 - value_mse: 0.0180\n",
            "Epoch 8/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1898 - policy_accuracy: 0.7374 - policy_loss: 1.1706 - val_loss: 1.2186 - val_policy_accuracy: 0.7320 - val_policy_loss: 1.1868 - val_value_loss: 0.0317 - val_value_mse: 0.0201 - value_loss: 0.0196 - value_mse: 0.0150\n",
            "Epoch 9/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1853 - policy_accuracy: 0.7412 - policy_loss: 1.1672 - val_loss: 1.2149 - val_policy_accuracy: 0.7336 - val_policy_loss: 1.1866 - val_value_loss: 0.0282 - val_value_mse: 0.0167 - value_loss: 0.0184 - value_mse: 0.0142\n",
            "Epoch 10/100\n",
            "2198/2198 - 19s - 8ms/step - loss: 1.1793 - policy_accuracy: 0.7466 - policy_loss: 1.1635 - val_loss: 1.2122 - val_policy_accuracy: 0.7393 - val_policy_loss: 1.1856 - val_value_loss: 0.0264 - val_value_mse: 0.0159 - value_loss: 0.0161 - value_mse: 0.0129\n",
            "Epoch 11/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1729 - policy_accuracy: 0.7516 - policy_loss: 1.1595 - val_loss: 1.2123 - val_policy_accuracy: 0.7390 - val_policy_loss: 1.1844 - val_value_loss: 0.0278 - val_value_mse: 0.0165 - value_loss: 0.0137 - value_mse: 0.0116\n",
            "Epoch 12/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1691 - policy_accuracy: 0.7550 - policy_loss: 1.1570 - val_loss: 1.2071 - val_policy_accuracy: 0.7445 - val_policy_loss: 1.1817 - val_value_loss: 0.0252 - val_value_mse: 0.0155 - value_loss: 0.0124 - value_mse: 0.0107\n",
            "Epoch 13/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1695 - policy_accuracy: 0.7559 - policy_loss: 1.1563 - val_loss: 1.2293 - val_policy_accuracy: 0.7330 - val_policy_loss: 1.1894 - val_value_loss: 0.0396 - val_value_mse: 0.0300 - value_loss: 0.0135 - value_mse: 0.0111\n",
            "Epoch 14/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1604 - policy_accuracy: 0.7633 - policy_loss: 1.1511 - val_loss: 1.1987 - val_policy_accuracy: 0.7529 - val_policy_loss: 1.1797 - val_value_loss: 0.0188 - val_value_mse: 0.0129 - value_loss: 0.0096 - value_mse: 0.0090\n",
            "Epoch 15/100\n",
            "2198/2198 - 19s - 8ms/step - loss: 1.1590 - policy_accuracy: 0.7645 - policy_loss: 1.1499 - val_loss: 1.2024 - val_policy_accuracy: 0.7530 - val_policy_loss: 1.1822 - val_value_loss: 0.0201 - val_value_mse: 0.0138 - value_loss: 0.0094 - value_mse: 0.0089\n",
            "Epoch 16/100\n",
            "2198/2198 - 19s - 8ms/step - loss: 1.1592 - policy_accuracy: 0.7654 - policy_loss: 1.1496 - val_loss: 1.2074 - val_policy_accuracy: 0.7499 - val_policy_loss: 1.1864 - val_value_loss: 0.0208 - val_value_mse: 0.0124 - value_loss: 0.0100 - value_mse: 0.0089\n",
            "Epoch 17/100\n",
            "2198/2198 - 19s - 8ms/step - loss: 1.1524 - policy_accuracy: 0.7710 - policy_loss: 1.1451 - val_loss: 1.1983 - val_policy_accuracy: 0.7589 - val_policy_loss: 1.1821 - val_value_loss: 0.0161 - val_value_mse: 0.0110 - value_loss: 0.0076 - value_mse: 0.0075\n",
            "Epoch 18/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1539 - policy_accuracy: 0.7707 - policy_loss: 1.1457 - val_loss: 1.2078 - val_policy_accuracy: 0.7525 - val_policy_loss: 1.1868 - val_value_loss: 0.0208 - val_value_mse: 0.0126 - value_loss: 0.0085 - value_mse: 0.0080\n",
            "Epoch 19/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1544 - policy_accuracy: 0.7712 - policy_loss: 1.1454 - val_loss: 1.2096 - val_policy_accuracy: 0.7564 - val_policy_loss: 1.1886 - val_value_loss: 0.0208 - val_value_mse: 0.0127 - value_loss: 0.0093 - value_mse: 0.0083\n",
            "Epoch 20/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1474 - policy_accuracy: 0.7772 - policy_loss: 1.1410 - val_loss: 1.2009 - val_policy_accuracy: 0.7630 - val_policy_loss: 1.1855 - val_value_loss: 0.0153 - val_value_mse: 0.0103 - value_loss: 0.0067 - value_mse: 0.0068\n",
            "Epoch 21/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1515 - policy_accuracy: 0.7744 - policy_loss: 1.1437 - val_loss: 1.2137 - val_policy_accuracy: 0.7534 - val_policy_loss: 1.1894 - val_value_loss: 0.0241 - val_value_mse: 0.0136 - value_loss: 0.0081 - value_mse: 0.0076\n",
            "Epoch 22/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1456 - policy_accuracy: 0.7808 - policy_loss: 1.1398 - val_loss: 1.2012 - val_policy_accuracy: 0.7649 - val_policy_loss: 1.1884 - val_value_loss: 0.0127 - val_value_mse: 0.0089 - value_loss: 0.0061 - value_mse: 0.0063\n",
            "Epoch 23/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1435 - policy_accuracy: 0.7826 - policy_loss: 1.1380 - val_loss: 1.1986 - val_policy_accuracy: 0.7624 - val_policy_loss: 1.1835 - val_value_loss: 0.0149 - val_value_mse: 0.0097 - value_loss: 0.0057 - value_mse: 0.0061\n",
            "Epoch 24/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1498 - policy_accuracy: 0.7779 - policy_loss: 1.1424 - val_loss: 1.2057 - val_policy_accuracy: 0.7595 - val_policy_loss: 1.1880 - val_value_loss: 0.0174 - val_value_mse: 0.0123 - value_loss: 0.0077 - value_mse: 0.0072\n",
            "Epoch 25/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1412 - policy_accuracy: 0.7859 - policy_loss: 1.1364 - val_loss: 1.2009 - val_policy_accuracy: 0.7673 - val_policy_loss: 1.1869 - val_value_loss: 0.0138 - val_value_mse: 0.0094 - value_loss: 0.0051 - value_mse: 0.0056\n",
            "Epoch 26/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1405 - policy_accuracy: 0.7876 - policy_loss: 1.1353 - val_loss: 1.2270 - val_policy_accuracy: 0.7460 - val_policy_loss: 1.1958 - val_value_loss: 0.0309 - val_value_mse: 0.0183 - value_loss: 0.0055 - value_mse: 0.0056\n",
            "Epoch 27/100\n",
            "2198/2198 - 18s - 8ms/step - loss: 1.1420 - policy_accuracy: 0.7865 - policy_loss: 1.1365 - val_loss: 1.2000 - val_policy_accuracy: 0.7655 - val_policy_loss: 1.1856 - val_value_loss: 0.0142 - val_value_mse: 0.0090 - value_loss: 0.0058 - value_mse: 0.0059\n",
            "Epoch 27: early stopping\n",
            "Restoring model weights from the end of the best epoch: 17.\n",
            "\n",
            "Model 3 summary:\n",
            "  Residual blocks  : 8\n",
            "  Filters          : 128\n",
            "  Learning rate    : 0.0003\n",
            "  Best epoch (val) : 17\n",
            "  Best val loss    : 1.1983\n",
            "  Train time (s)   : 525.0\n",
            "\n",
            "======================================================================\n",
            "Training model 4/4 | config = {'num_blocks': 10, 'learning_rate': 0.0003}\n",
            "======================================================================\n",
            "Epoch 1/100\n",
            "2198/2198 - 57s - 26ms/step - loss: 1.6033 - policy_accuracy: 0.5200 - policy_loss: 1.4105 - val_loss: 1.4725 - val_policy_accuracy: 0.5839 - val_policy_loss: 1.3284 - val_value_loss: 0.1437 - val_value_mse: 0.0761 - value_loss: 0.1931 - value_mse: 0.1054\n",
            "Epoch 2/100\n",
            "2198/2198 - 21s - 10ms/step - loss: 1.3701 - policy_accuracy: 0.6477 - policy_loss: 1.2579 - val_loss: 1.3572 - val_policy_accuracy: 0.6547 - val_policy_loss: 1.2584 - val_value_loss: 0.0985 - val_value_mse: 0.0501 - value_loss: 0.1126 - value_mse: 0.0545\n",
            "Epoch 3/100\n",
            "2198/2198 - 21s - 10ms/step - loss: 1.2927 - policy_accuracy: 0.6848 - policy_loss: 1.2191 - val_loss: 1.2962 - val_policy_accuracy: 0.6879 - val_policy_loss: 1.2216 - val_value_loss: 0.0743 - val_value_mse: 0.0481 - value_loss: 0.0739 - value_mse: 0.0387\n",
            "Epoch 4/100\n",
            "2198/2198 - 22s - 10ms/step - loss: 1.2486 - policy_accuracy: 0.7032 - policy_loss: 1.2008 - val_loss: 1.2640 - val_policy_accuracy: 0.7026 - val_policy_loss: 1.2120 - val_value_loss: 0.0518 - val_value_mse: 0.0286 - value_loss: 0.0481 - value_mse: 0.0286\n",
            "Epoch 5/100\n",
            "2198/2198 - 21s - 10ms/step - loss: 1.2216 - policy_accuracy: 0.7182 - policy_loss: 1.1879 - val_loss: 1.2328 - val_policy_accuracy: 0.7219 - val_policy_loss: 1.1958 - val_value_loss: 0.0368 - val_value_mse: 0.0227 - value_loss: 0.0340 - value_mse: 0.0221\n",
            "Epoch 6/100\n",
            "2198/2198 - 21s - 10ms/step - loss: 1.2076 - policy_accuracy: 0.7271 - policy_loss: 1.1811 - val_loss: 1.2156 - val_policy_accuracy: 0.7301 - val_policy_loss: 1.1873 - val_value_loss: 0.0281 - val_value_mse: 0.0175 - value_loss: 0.0268 - value_mse: 0.0188\n",
            "Epoch 7/100\n",
            "2198/2198 - 21s - 10ms/step - loss: 1.1923 - policy_accuracy: 0.7369 - policy_loss: 1.1730 - val_loss: 1.2421 - val_policy_accuracy: 0.7216 - val_policy_loss: 1.1987 - val_value_loss: 0.0433 - val_value_mse: 0.0298 - value_loss: 0.0196 - value_mse: 0.0150\n",
            "Epoch 8/100\n",
            "2198/2198 - 21s - 10ms/step - loss: 1.1884 - policy_accuracy: 0.7407 - policy_loss: 1.1702 - val_loss: 1.2367 - val_policy_accuracy: 0.7305 - val_policy_loss: 1.1924 - val_value_loss: 0.0441 - val_value_mse: 0.0364 - value_loss: 0.0185 - value_mse: 0.0143\n",
            "Epoch 9/100\n",
            "2198/2198 - 21s - 10ms/step - loss: 1.1808 - policy_accuracy: 0.7471 - policy_loss: 1.1655 - val_loss: 1.2026 - val_policy_accuracy: 0.7437 - val_policy_loss: 1.1804 - val_value_loss: 0.0220 - val_value_mse: 0.0139 - value_loss: 0.0156 - value_mse: 0.0127\n",
            "Epoch 10/100\n",
            "2198/2198 - 21s - 10ms/step - loss: 1.1747 - policy_accuracy: 0.7523 - policy_loss: 1.1612 - val_loss: 1.2183 - val_policy_accuracy: 0.7343 - val_policy_loss: 1.1884 - val_value_loss: 0.0297 - val_value_mse: 0.0173 - value_loss: 0.0139 - value_mse: 0.0114\n",
            "Epoch 11/100\n",
            "2198/2198 - 21s - 10ms/step - loss: 1.1743 - policy_accuracy: 0.7544 - policy_loss: 1.1599 - val_loss: 1.1939 - val_policy_accuracy: 0.7477 - val_policy_loss: 1.1769 - val_value_loss: 0.0169 - val_value_mse: 0.0115 - value_loss: 0.0146 - value_mse: 0.0115\n",
            "Epoch 12/100\n",
            "2198/2198 - 21s - 10ms/step - loss: 1.1634 - policy_accuracy: 0.7620 - policy_loss: 1.1544 - val_loss: 1.2139 - val_policy_accuracy: 0.7438 - val_policy_loss: 1.1835 - val_value_loss: 0.0303 - val_value_mse: 0.0186 - value_loss: 0.0093 - value_mse: 0.0089\n",
            "Epoch 13/100\n",
            "2198/2198 - 22s - 10ms/step - loss: 1.1664 - policy_accuracy: 0.7606 - policy_loss: 1.1550 - val_loss: 1.1908 - val_policy_accuracy: 0.7548 - val_policy_loss: 1.1752 - val_value_loss: 0.0154 - val_value_mse: 0.0103 - value_loss: 0.0117 - value_mse: 0.0100\n",
            "Epoch 14/100\n",
            "2198/2198 - 21s - 10ms/step - loss: 1.1552 - policy_accuracy: 0.7696 - policy_loss: 1.1482 - val_loss: 1.2030 - val_policy_accuracy: 0.7461 - val_policy_loss: 1.1823 - val_value_loss: 0.0205 - val_value_mse: 0.0135 - value_loss: 0.0073 - value_mse: 0.0074\n",
            "Epoch 15/100\n",
            "2198/2198 - 21s - 10ms/step - loss: 1.1568 - policy_accuracy: 0.7688 - policy_loss: 1.1483 - val_loss: 1.2014 - val_policy_accuracy: 0.7545 - val_policy_loss: 1.1789 - val_value_loss: 0.0223 - val_value_mse: 0.0158 - value_loss: 0.0088 - value_mse: 0.0081\n",
            "Epoch 16/100\n",
            "2198/2198 - 21s - 10ms/step - loss: 1.1525 - policy_accuracy: 0.7732 - policy_loss: 1.1455 - val_loss: 1.1969 - val_policy_accuracy: 0.7546 - val_policy_loss: 1.1790 - val_value_loss: 0.0177 - val_value_mse: 0.0132 - value_loss: 0.0074 - value_mse: 0.0073\n",
            "Epoch 17/100\n",
            "2198/2198 - 21s - 10ms/step - loss: 1.1511 - policy_accuracy: 0.7751 - policy_loss: 1.1446 - val_loss: 1.1901 - val_policy_accuracy: 0.7621 - val_policy_loss: 1.1761 - val_value_loss: 0.0139 - val_value_mse: 0.0097 - value_loss: 0.0068 - value_mse: 0.0070\n",
            "Epoch 18/100\n",
            "2198/2198 - 22s - 10ms/step - loss: 1.1505 - policy_accuracy: 0.7766 - policy_loss: 1.1435 - val_loss: 1.1919 - val_policy_accuracy: 0.7624 - val_policy_loss: 1.1757 - val_value_loss: 0.0160 - val_value_mse: 0.0125 - value_loss: 0.0073 - value_mse: 0.0071\n",
            "Epoch 19/100\n",
            "2198/2198 - 21s - 10ms/step - loss: 1.1492 - policy_accuracy: 0.7784 - policy_loss: 1.1423 - val_loss: 1.1956 - val_policy_accuracy: 0.7651 - val_policy_loss: 1.1800 - val_value_loss: 0.0155 - val_value_mse: 0.0103 - value_loss: 0.0072 - value_mse: 0.0069\n",
            "Epoch 20/100\n",
            "2198/2198 - 21s - 10ms/step - loss: 1.1447 - policy_accuracy: 0.7826 - policy_loss: 1.1396 - val_loss: 1.2067 - val_policy_accuracy: 0.7497 - val_policy_loss: 1.1830 - val_value_loss: 0.0235 - val_value_mse: 0.0134 - value_loss: 0.0054 - value_mse: 0.0059\n",
            "Epoch 21/100\n",
            "2198/2198 - 21s - 10ms/step - loss: 1.1473 - policy_accuracy: 0.7814 - policy_loss: 1.1411 - val_loss: 1.1907 - val_policy_accuracy: 0.7678 - val_policy_loss: 1.1780 - val_value_loss: 0.0126 - val_value_mse: 0.0091 - value_loss: 0.0065 - value_mse: 0.0064\n",
            "Epoch 22/100\n",
            "2198/2198 - 22s - 10ms/step - loss: 1.1432 - policy_accuracy: 0.7856 - policy_loss: 1.1381 - val_loss: 1.1886 - val_policy_accuracy: 0.7695 - val_policy_loss: 1.1775 - val_value_loss: 0.0109 - val_value_mse: 0.0079 - value_loss: 0.0053 - value_mse: 0.0056\n",
            "Epoch 23/100\n",
            "2198/2198 - 21s - 10ms/step - loss: 1.1437 - policy_accuracy: 0.7852 - policy_loss: 1.1384 - val_loss: 1.1954 - val_policy_accuracy: 0.7619 - val_policy_loss: 1.1796 - val_value_loss: 0.0156 - val_value_mse: 0.0115 - value_loss: 0.0055 - value_mse: 0.0057\n",
            "Epoch 24/100\n",
            "2198/2198 - 22s - 10ms/step - loss: 1.1419 - policy_accuracy: 0.7875 - policy_loss: 1.1369 - val_loss: 1.1899 - val_policy_accuracy: 0.7697 - val_policy_loss: 1.1787 - val_value_loss: 0.0110 - val_value_mse: 0.0077 - value_loss: 0.0053 - value_mse: 0.0055\n",
            "Epoch 25/100\n",
            "2198/2198 - 22s - 10ms/step - loss: 1.1389 - policy_accuracy: 0.7906 - policy_loss: 1.1347 - val_loss: 1.1940 - val_policy_accuracy: 0.7657 - val_policy_loss: 1.1774 - val_value_loss: 0.0164 - val_value_mse: 0.0104 - value_loss: 0.0045 - value_mse: 0.0050\n",
            "Epoch 26/100\n",
            "2198/2198 - 21s - 10ms/step - loss: 1.1441 - policy_accuracy: 0.7870 - policy_loss: 1.1381 - val_loss: 1.1943 - val_policy_accuracy: 0.7702 - val_policy_loss: 1.1826 - val_value_loss: 0.0115 - val_value_mse: 0.0078 - value_loss: 0.0063 - value_mse: 0.0061\n",
            "Epoch 27/100\n",
            "2198/2198 - 21s - 10ms/step - loss: 1.1391 - policy_accuracy: 0.7919 - policy_loss: 1.1342 - val_loss: 1.1978 - val_policy_accuracy: 0.7666 - val_policy_loss: 1.1830 - val_value_loss: 0.0146 - val_value_mse: 0.0098 - value_loss: 0.0053 - value_mse: 0.0053\n",
            "Epoch 28/100\n",
            "2198/2198 - 22s - 10ms/step - loss: 1.1389 - policy_accuracy: 0.7925 - policy_loss: 1.1346 - val_loss: 1.1897 - val_policy_accuracy: 0.7738 - val_policy_loss: 1.1794 - val_value_loss: 0.0101 - val_value_mse: 0.0075 - value_loss: 0.0047 - value_mse: 0.0051\n",
            "Epoch 29/100\n",
            "2198/2198 - 22s - 10ms/step - loss: 1.1380 - policy_accuracy: 0.7943 - policy_loss: 1.1337 - val_loss: 1.1914 - val_policy_accuracy: 0.7732 - val_policy_loss: 1.1810 - val_value_loss: 0.0102 - val_value_mse: 0.0078 - value_loss: 0.0046 - value_mse: 0.0050\n",
            "Epoch 30/100\n",
            "2198/2198 - 21s - 10ms/step - loss: 1.1359 - policy_accuracy: 0.7962 - policy_loss: 1.1324 - val_loss: 1.1881 - val_policy_accuracy: 0.7676 - val_policy_loss: 1.1752 - val_value_loss: 0.0128 - val_value_mse: 0.0091 - value_loss: 0.0038 - value_mse: 0.0044\n",
            "Epoch 31/100\n",
            "2198/2198 - 22s - 10ms/step - loss: 1.1393 - policy_accuracy: 0.7934 - policy_loss: 1.1348 - val_loss: 1.1875 - val_policy_accuracy: 0.7719 - val_policy_loss: 1.1771 - val_value_loss: 0.0103 - val_value_mse: 0.0073 - value_loss: 0.0049 - value_mse: 0.0051\n",
            "Epoch 32/100\n",
            "2198/2198 - 22s - 10ms/step - loss: 1.1363 - policy_accuracy: 0.7974 - policy_loss: 1.1321 - val_loss: 1.1977 - val_policy_accuracy: 0.7664 - val_policy_loss: 1.1837 - val_value_loss: 0.0138 - val_value_mse: 0.0102 - value_loss: 0.0046 - value_mse: 0.0048\n",
            "Epoch 33/100\n",
            "2198/2198 - 22s - 10ms/step - loss: 1.1324 - policy_accuracy: 0.8010 - policy_loss: 1.1295 - val_loss: 1.1910 - val_policy_accuracy: 0.7743 - val_policy_loss: 1.1806 - val_value_loss: 0.0102 - val_value_mse: 0.0080 - value_loss: 0.0032 - value_mse: 0.0040\n",
            "Epoch 34/100\n",
            "2198/2198 - 22s - 10ms/step - loss: 1.1352 - policy_accuracy: 0.7988 - policy_loss: 1.1312 - val_loss: 1.2050 - val_policy_accuracy: 0.7668 - val_policy_loss: 1.1867 - val_value_loss: 0.0181 - val_value_mse: 0.0135 - value_loss: 0.0043 - value_mse: 0.0045\n",
            "Epoch 35/100\n",
            "2198/2198 - 22s - 10ms/step - loss: 1.1313 - policy_accuracy: 0.8035 - policy_loss: 1.1286 - val_loss: 1.2047 - val_policy_accuracy: 0.7635 - val_policy_loss: 1.1915 - val_value_loss: 0.0131 - val_value_mse: 0.0091 - value_loss: 0.0029 - value_mse: 0.0037\n",
            "Epoch 36/100\n",
            "2198/2198 - 21s - 10ms/step - loss: 1.1367 - policy_accuracy: 0.7981 - policy_loss: 1.1319 - val_loss: 1.2019 - val_policy_accuracy: 0.7689 - val_policy_loss: 1.1901 - val_value_loss: 0.0116 - val_value_mse: 0.0082 - value_loss: 0.0052 - value_mse: 0.0051\n",
            "Epoch 37/100\n",
            "2198/2198 - 21s - 10ms/step - loss: 1.1346 - policy_accuracy: 0.8005 - policy_loss: 1.1308 - val_loss: 1.1894 - val_policy_accuracy: 0.7756 - val_policy_loss: 1.1797 - val_value_loss: 0.0096 - val_value_mse: 0.0066 - value_loss: 0.0041 - value_mse: 0.0044\n",
            "Epoch 38/100\n",
            "2198/2198 - 21s - 10ms/step - loss: 1.1288 - policy_accuracy: 0.8075 - policy_loss: 1.1266 - val_loss: 1.1926 - val_policy_accuracy: 0.7767 - val_policy_loss: 1.1838 - val_value_loss: 0.0086 - val_value_mse: 0.0063 - value_loss: 0.0025 - value_mse: 0.0034\n",
            "Epoch 39/100\n",
            "2198/2198 - 21s - 10ms/step - loss: 1.1347 - policy_accuracy: 0.8007 - policy_loss: 1.1306 - val_loss: 1.1925 - val_policy_accuracy: 0.7739 - val_policy_loss: 1.1820 - val_value_loss: 0.0104 - val_value_mse: 0.0071 - value_loss: 0.0044 - value_mse: 0.0046\n",
            "Epoch 40/100\n",
            "2198/2198 - 21s - 10ms/step - loss: 1.1304 - policy_accuracy: 0.8064 - policy_loss: 1.1278 - val_loss: 1.1953 - val_policy_accuracy: 0.7771 - val_policy_loss: 1.1862 - val_value_loss: 0.0090 - val_value_mse: 0.0067 - value_loss: 0.0029 - value_mse: 0.0036\n",
            "Epoch 41/100\n",
            "2198/2198 - 21s - 10ms/step - loss: 1.1282 - policy_accuracy: 0.8093 - policy_loss: 1.1261 - val_loss: 1.1925 - val_policy_accuracy: 0.7743 - val_policy_loss: 1.1829 - val_value_loss: 0.0094 - val_value_mse: 0.0072 - value_loss: 0.0024 - value_mse: 0.0032\n",
            "Epoch 41: early stopping\n",
            "Restoring model weights from the end of the best epoch: 31.\n",
            "\n",
            "Model 4 summary:\n",
            "  Residual blocks  : 10\n",
            "  Filters          : 128\n",
            "  Learning rate    : 0.0003\n",
            "  Best epoch (val) : 31\n",
            "  Best val loss    : 1.1875\n",
            "  Train time (s)   : 914.9\n",
            "\n",
            "=== MODEL COMPARISON (VALIDATION / TEST) ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   model_id  num_blocks  filters  learning_rate  best_epoch_val  \\\n",
              "0         2          10      128         0.0010              12   \n",
              "1         1           8      128         0.0010              32   \n",
              "2         4          10      128         0.0003              31   \n",
              "3         3           8      128         0.0003              17   \n",
              "\n",
              "   best_val_loss  train_time_sec  \n",
              "0       1.181843      493.142571  \n",
              "1       1.183266      804.454851  \n",
              "2       1.187543      914.900691  \n",
              "3       1.198297      524.967894  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f9a7b461-5acb-419c-8e13-a4f55e9ca9a2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_id</th>\n",
              "      <th>num_blocks</th>\n",
              "      <th>filters</th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>best_epoch_val</th>\n",
              "      <th>best_val_loss</th>\n",
              "      <th>train_time_sec</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>128</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>12</td>\n",
              "      <td>1.181843</td>\n",
              "      <td>493.142571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>128</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>32</td>\n",
              "      <td>1.183266</td>\n",
              "      <td>804.454851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>128</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>31</td>\n",
              "      <td>1.187543</td>\n",
              "      <td>914.900691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>128</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>17</td>\n",
              "      <td>1.198297</td>\n",
              "      <td>524.967894</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f9a7b461-5acb-419c-8e13-a4f55e9ca9a2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f9a7b461-5acb-419c-8e13-a4f55e9ca9a2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f9a7b461-5acb-419c-8e13-a4f55e9ca9a2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_9130817e-91a0-4c7f-be72-b107de8f8387\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_9130817e-91a0-4c7f-be72-b107de8f8387 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_df",
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"model_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          3,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_blocks\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 8,\n        \"max\": 10,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          8,\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"filters\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 128,\n        \"max\": 128,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          128\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"learning_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00040414518843273807,\n        \"min\": 0.0003,\n        \"max\": 0.001,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0003\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_epoch_val\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10,\n        \"min\": 12,\n        \"max\": 32,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_val_loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.007444821045269065,\n        \"min\": 1.1818432807922363,\n        \"max\": 1.1982970237731934,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1.1832661628723145\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_time_sec\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 207.79936812401934,\n        \"min\": 493.14257073402405,\n        \"max\": 914.9006912708282,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          804.4548513889313\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved artifacts:\n",
            "  Results table     → /content/drive/MyDrive/Colab Notebooks/connect-4/results/model_grid_results.csv\n",
            "  Training histories→ /content/drive/MyDrive/Colab Notebooks/connect-4/results/training_histories.pkl\n",
            "  Best model        → /content/drive/MyDrive/Colab Notebooks/connect-4/models/best_model.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7 — Targeted Capacity Probe (Model 2 + Wider Network)\n",
        "\n",
        "In this step we run a **single, focused experiment** to test whether additional\n",
        "representational capacity improves performance.\n",
        "\n",
        "### Motivation\n",
        "\n",
        "From Step 6, **Model 2** emerged as the best-performing architecture:\n",
        "- 10 residual blocks\n",
        "- 128 filters\n",
        "- learning rate = 1e-3\n",
        "- fast convergence and lowest validation loss\n",
        "\n",
        "This suggests the model may be **capacity-limited**, not optimization-limited.\n",
        "\n",
        "### What we change (and only this)\n",
        "\n",
        "- Increase network width:\n",
        "  - **filters: 128 → 256**\n",
        "- Reduce batch size to maintain stability and avoid OOM:\n",
        "  - **batch size: 512 → 256**\n",
        "\n",
        "### What we keep fixed\n",
        "\n",
        "- Architecture (residual structure, heads)\n",
        "- Number of residual blocks (10)\n",
        "- Learning rate (1e-3)\n",
        "- Loss functions and move-depth weighting\n",
        "- TRAIN / TEST split\n",
        "- Early stopping configuration\n",
        "\n",
        "### Evaluation\n",
        "\n",
        "- Train the wider model using TEST as validation (same as Step 6)\n",
        "- Load the previously saved `best_model.keras`\n",
        "- Compare:\n",
        "  - Best validation loss (primary)\n",
        "  - Convergence behavior (epoch count)\n",
        "\n",
        "This isolates the effect of **width alone** and tells us whether a wider network\n",
        "is worth adopting before moving to self-play or further architectural changes."
      ],
      "metadata": {
        "id": "s_pAXsNtM0aj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# STEP 7 — TRAIN WIDER MODEL (MODEL 2 + 256 FILTERS)\n",
        "# + COMPARE AGAINST SAVED BEST MODEL\n",
        "# ======================================================\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# Configuration (ONLY width & batch size change)\n",
        "# ------------------------------------------------------\n",
        "\n",
        "NUM_BLOCKS = 10           # same as Model 2\n",
        "LEARNING_RATE = 1e-3      # same as Model 2\n",
        "FILTERS_WIDE = 256        # wider network\n",
        "BATCH_SIZE_WIDE = 256     # reduced batch size\n",
        "EPOCHS_MAX = 100\n",
        "PATIENCE = 10\n",
        "\n",
        "print(\"\\nRunning targeted capacity probe:\")\n",
        "print(f\"  Residual blocks : {NUM_BLOCKS}\")\n",
        "print(f\"  Filters         : {FILTERS_WIDE}\")\n",
        "print(f\"  Learning rate   : {LEARNING_RATE}\")\n",
        "print(f\"  Batch size      : {BATCH_SIZE_WIDE}\")\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# Build wide model\n",
        "# ------------------------------------------------------\n",
        "\n",
        "wide_model = build_residual_cnn(\n",
        "    input_shape=(6, 7, 2),\n",
        "    num_blocks=NUM_BLOCKS,\n",
        "    filters=FILTERS_WIDE,\n",
        "    learning_rate=LEARNING_RATE,\n",
        ")\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=PATIENCE,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# Train\n",
        "# ------------------------------------------------------\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "wide_history = wide_model.fit(\n",
        "    X_train,\n",
        "    [policy_train, value_train],\n",
        "    sample_weight=[w_train, w_train],\n",
        "    validation_data=(\n",
        "        X_test,\n",
        "        [policy_test, value_test],\n",
        "        [w_test, w_test],\n",
        "    ),\n",
        "    epochs=EPOCHS_MAX,\n",
        "    batch_size=BATCH_SIZE_WIDE,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=2,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "\n",
        "best_epoch_wide = int(np.argmin(wide_history.history[\"val_loss\"]) + 1)\n",
        "best_val_loss_wide = float(np.min(wide_history.history[\"val_loss\"]))\n",
        "\n",
        "print(\"\\nWide model summary:\")\n",
        "print(f\"  Best epoch (val) : {best_epoch_wide}\")\n",
        "print(f\"  Best val loss    : {best_val_loss_wide:.6f}\")\n",
        "print(f\"  Train time (s)   : {elapsed:.1f}\")\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# Load previously saved best model (from Step 6)\n",
        "# ------------------------------------------------------\n",
        "\n",
        "best_model_path = os.path.join(MODELS_DIR, \"best_model.keras\")\n",
        "best_model = tf.keras.models.load_model(best_model_path)\n",
        "\n",
        "best_model_val = best_model.evaluate(\n",
        "    X_test,\n",
        "    [policy_test, value_test],\n",
        "    sample_weight=[w_test, w_test],\n",
        "    verbose=0,\n",
        ")\n",
        "\n",
        "best_model_val_loss = float(best_model_val[0])\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# Comparison\n",
        "# ------------------------------------------------------\n",
        "\n",
        "print(\"\\n=== VALIDATION LOSS COMPARISON ===\")\n",
        "print(f\"  Previous best model (128 filters): {best_model_val_loss:.6f}\")\n",
        "print(f\"  Wide model (256 filters)          : {best_val_loss_wide:.6f}\")\n",
        "\n",
        "delta = best_val_loss_wide - best_model_val_loss\n",
        "print(f\"\\n  Δ val_loss (wide - previous) = {delta:+.6f}\")\n",
        "\n",
        "if delta < 0:\n",
        "    print(\"Wider model improves validation loss.\")\n",
        "else:\n",
        "    print(\"Wider model does NOT improve validation loss.\")\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# Optional: save wide model if it wins\n",
        "# ------------------------------------------------------\n",
        "\n",
        "if delta < 0:\n",
        "    wide_model_path = os.path.join(MODELS_DIR, \"best_model_256f.keras\")\n",
        "    wide_model.save(wide_model_path)\n",
        "    print(f\"\\nSaved improved wide model → {wide_model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBAjtuxiaYiJ",
        "outputId": "56c905b0-287b-4c49-b2a4-084720d7420a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running targeted capacity probe:\n",
            "  Residual blocks : 10\n",
            "  Filters         : 256\n",
            "  Learning rate   : 0.001\n",
            "  Batch size      : 256\n",
            "Epoch 1/100\n",
            "4395/4395 - 124s - 28ms/step - loss: 1.5364 - policy_accuracy: 0.5590 - policy_loss: 1.3662 - val_loss: 1.4283 - val_policy_accuracy: 0.6682 - val_policy_loss: 1.2422 - val_value_loss: 0.1860 - val_value_mse: 0.1265 - value_loss: 0.1703 - value_mse: 0.0898\n",
            "Epoch 2/100\n",
            "4395/4395 - 53s - 12ms/step - loss: 1.2897 - policy_accuracy: 0.6925 - policy_loss: 1.2172 - val_loss: 1.2481 - val_policy_accuracy: 0.7157 - val_policy_loss: 1.2000 - val_value_loss: 0.0480 - val_value_mse: 0.0251 - value_loss: 0.0726 - value_mse: 0.0380\n",
            "Epoch 3/100\n",
            "4395/4395 - 52s - 12ms/step - loss: 1.2275 - policy_accuracy: 0.7229 - policy_loss: 1.1898 - val_loss: 1.2293 - val_policy_accuracy: 0.7349 - val_policy_loss: 1.1878 - val_value_loss: 0.0414 - val_value_mse: 0.0272 - value_loss: 0.0378 - value_mse: 0.0231\n",
            "Epoch 4/100\n",
            "4395/4395 - 52s - 12ms/step - loss: 1.2026 - policy_accuracy: 0.7380 - policy_loss: 1.1774 - val_loss: 1.2034 - val_policy_accuracy: 0.7446 - val_policy_loss: 1.1784 - val_value_loss: 0.0250 - val_value_mse: 0.0163 - value_loss: 0.0253 - value_mse: 0.0172\n",
            "Epoch 5/100\n",
            "4395/4395 - 53s - 12ms/step - loss: 1.1847 - policy_accuracy: 0.7509 - policy_loss: 1.1678 - val_loss: 1.2068 - val_policy_accuracy: 0.7449 - val_policy_loss: 1.1800 - val_value_loss: 0.0267 - val_value_mse: 0.0171 - value_loss: 0.0170 - value_mse: 0.0129\n",
            "Epoch 6/100\n",
            "4395/4395 - 53s - 12ms/step - loss: 1.1760 - policy_accuracy: 0.7581 - policy_loss: 1.1621 - val_loss: 1.1900 - val_policy_accuracy: 0.7593 - val_policy_loss: 1.1702 - val_value_loss: 0.0197 - val_value_mse: 0.0167 - value_loss: 0.0140 - value_mse: 0.0110\n",
            "Epoch 7/100\n",
            "4395/4395 - 53s - 12ms/step - loss: 1.1682 - policy_accuracy: 0.7649 - policy_loss: 1.1573 - val_loss: 1.1807 - val_policy_accuracy: 0.7650 - val_policy_loss: 1.1684 - val_value_loss: 0.0122 - val_value_mse: 0.0088 - value_loss: 0.0110 - value_mse: 0.0092\n",
            "Epoch 8/100\n",
            "4395/4395 - 55s - 12ms/step - loss: 1.1625 - policy_accuracy: 0.7694 - policy_loss: 1.1532 - val_loss: 1.1800 - val_policy_accuracy: 0.7679 - val_policy_loss: 1.1679 - val_value_loss: 0.0121 - val_value_mse: 0.0090 - value_loss: 0.0094 - value_mse: 0.0082\n",
            "Epoch 9/100\n",
            "4395/4395 - 53s - 12ms/step - loss: 1.1585 - policy_accuracy: 0.7734 - policy_loss: 1.1501 - val_loss: 1.1779 - val_policy_accuracy: 0.7711 - val_policy_loss: 1.1675 - val_value_loss: 0.0104 - val_value_mse: 0.0085 - value_loss: 0.0085 - value_mse: 0.0077\n",
            "Epoch 10/100\n",
            "4395/4395 - 54s - 12ms/step - loss: 1.1530 - policy_accuracy: 0.7780 - policy_loss: 1.1463 - val_loss: 1.1796 - val_policy_accuracy: 0.7708 - val_policy_loss: 1.1679 - val_value_loss: 0.0116 - val_value_mse: 0.0097 - value_loss: 0.0068 - value_mse: 0.0065\n",
            "Epoch 11/100\n",
            "4395/4395 - 53s - 12ms/step - loss: 1.1511 - policy_accuracy: 0.7804 - policy_loss: 1.1445 - val_loss: 1.1821 - val_policy_accuracy: 0.7690 - val_policy_loss: 1.1716 - val_value_loss: 0.0105 - val_value_mse: 0.0079 - value_loss: 0.0068 - value_mse: 0.0065\n",
            "Epoch 12/100\n",
            "4395/4395 - 53s - 12ms/step - loss: 1.1474 - policy_accuracy: 0.7838 - policy_loss: 1.1418 - val_loss: 1.1821 - val_policy_accuracy: 0.7717 - val_policy_loss: 1.1700 - val_value_loss: 0.0120 - val_value_mse: 0.0086 - value_loss: 0.0057 - value_mse: 0.0058\n",
            "Epoch 13/100\n",
            "4395/4395 - 52s - 12ms/step - loss: 1.1458 - policy_accuracy: 0.7862 - policy_loss: 1.1401 - val_loss: 1.1867 - val_policy_accuracy: 0.7694 - val_policy_loss: 1.1722 - val_value_loss: 0.0144 - val_value_mse: 0.0110 - value_loss: 0.0058 - value_mse: 0.0057\n",
            "Epoch 14/100\n",
            "4395/4395 - 53s - 12ms/step - loss: 1.1433 - policy_accuracy: 0.7892 - policy_loss: 1.1383 - val_loss: 1.1897 - val_policy_accuracy: 0.7722 - val_policy_loss: 1.1759 - val_value_loss: 0.0138 - val_value_mse: 0.0093 - value_loss: 0.0051 - value_mse: 0.0052\n",
            "Epoch 15/100\n",
            "4395/4395 - 53s - 12ms/step - loss: 1.1417 - policy_accuracy: 0.7910 - policy_loss: 1.1374 - val_loss: 1.1781 - val_policy_accuracy: 0.7776 - val_policy_loss: 1.1691 - val_value_loss: 0.0090 - val_value_mse: 0.0068 - value_loss: 0.0044 - value_mse: 0.0048\n",
            "Epoch 16/100\n",
            "4395/4395 - 52s - 12ms/step - loss: 1.1397 - policy_accuracy: 0.7935 - policy_loss: 1.1356 - val_loss: 1.1788 - val_policy_accuracy: 0.7787 - val_policy_loss: 1.1693 - val_value_loss: 0.0095 - val_value_mse: 0.0072 - value_loss: 0.0042 - value_mse: 0.0047\n",
            "Epoch 17/100\n",
            "4395/4395 - 52s - 12ms/step - loss: 1.1412 - policy_accuracy: 0.7929 - policy_loss: 1.1361 - val_loss: 1.1819 - val_policy_accuracy: 0.7784 - val_policy_loss: 1.1731 - val_value_loss: 0.0087 - val_value_mse: 0.0067 - value_loss: 0.0051 - value_mse: 0.0051\n",
            "Epoch 18/100\n",
            "4395/4395 - 52s - 12ms/step - loss: 1.1386 - policy_accuracy: 0.7955 - policy_loss: 1.1343 - val_loss: 1.1820 - val_policy_accuracy: 0.7792 - val_policy_loss: 1.1728 - val_value_loss: 0.0091 - val_value_mse: 0.0068 - value_loss: 0.0044 - value_mse: 0.0047\n",
            "Epoch 19/100\n",
            "4395/4395 - 52s - 12ms/step - loss: 1.1365 - policy_accuracy: 0.7984 - policy_loss: 1.1329 - val_loss: 1.1849 - val_policy_accuracy: 0.7814 - val_policy_loss: 1.1770 - val_value_loss: 0.0078 - val_value_mse: 0.0060 - value_loss: 0.0037 - value_mse: 0.0041\n",
            "Epoch 19: early stopping\n",
            "Restoring model weights from the end of the best epoch: 9.\n",
            "\n",
            "Wide model summary:\n",
            "  Best epoch (val) : 9\n",
            "  Best val loss    : 1.177888\n",
            "  Train time (s)   : 1075.4\n",
            "\n",
            "=== VALIDATION LOSS COMPARISON ===\n",
            "  Previous best model (128 filters): 1.181844\n",
            "  Wide model (256 filters)          : 1.177888\n",
            "\n",
            "  Δ val_loss (wide - previous) = -0.003956\n",
            "Wider model improves validation loss.\n",
            "\n",
            "Saved improved wide model → /content/drive/MyDrive/Colab Notebooks/connect-4/models/best_model_256f.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7 — Post-Training Diagnostics & Model Comparison\n",
        "\n",
        "At this point, we have trained and saved multiple AlphaZero-style\n",
        "policy/value networks using the same dataset, loss weighting, and\n",
        "training protocol.\n",
        "\n",
        "Two candidate models are now available:\n",
        "- A baseline model (128 filters)\n",
        "- A wider model (256 filters)\n",
        "\n",
        "The validation loss difference between these models is real but small,\n",
        "suggesting that **model capacity is no longer the dominant bottleneck**.\n",
        "\n",
        "Before training larger or more complex architectures, we now shift from\n",
        "*model search* to *model diagnosis*.\n",
        "\n",
        "The purpose of this step is to understand **what the models have learned,\n",
        "where they differ, and what signal they may be missing**.\n",
        "\n",
        "---\n",
        "\n",
        "### Why we pause architecture changes here\n",
        "\n",
        "Adding more layers, filters, or dense heads only helps if:\n",
        "- the model is under-parameterized **relative to the learning signal**\n",
        "- or the model is failing to fit clear structure in the data\n",
        "\n",
        "A small improvement from doubling filters suggests:\n",
        "- the network can use extra capacity\n",
        "- but the **supervision signal may be too coarse**, especially early in the game\n",
        "\n",
        "This is a classic AlphaZero failure mode:  \n",
        "the network learns late-game outcomes well, but struggles to extract\n",
        "useful value signal from early or ambiguous positions.\n",
        "\n",
        "---\n",
        "\n",
        "### What is `Q` and why it matters\n",
        "\n",
        "In this dataset, each position includes a `Q` value from MCTS.\n",
        "\n",
        "Conceptually:\n",
        "- **`value`** is typically a game outcome target  \n",
        "  (e.g., win/loss or final result propagated back)\n",
        "- **`Q`** is the **expected value estimate from search**, averaged over simulations\n",
        "\n",
        "Key difference:\n",
        "- `value` answers: *“Who eventually won?”*\n",
        "- `Q` answers: *“How good is this position according to search?”*\n",
        "\n",
        "In AlphaZero-style systems:\n",
        "- `Q` is often a **richer, lower-variance signal**\n",
        "- especially for early-game positions where outcomes are far away\n",
        "\n",
        "Using `Q` (or a blend of `Q` and outcome value) as the value target can:\n",
        "- improve value calibration\n",
        "- stabilize early-game learning\n",
        "- reduce noisy gradients from distant outcomes\n",
        "\n",
        "However, this should **only be done after confirming** that:\n",
        "- the current value head is miscalibrated\n",
        "- or poorly correlated with search estimates\n",
        "\n",
        "---\n",
        "\n",
        "### What we will do next (no retraining yet)\n",
        "\n",
        "We will **evaluate and compare the two trained models** on the same\n",
        "validation set along three diagnostic axes:\n",
        "\n",
        "1. **Value calibration**\n",
        "   - Correlation between model value output and MCTS `Q`\n",
        "   - Error by move depth (early / mid / late game)\n",
        "\n",
        "2. **Policy sharpness**\n",
        "   - Entropy of predicted policy distributions\n",
        "   - KL divergence vs MCTS visit distribution (if available)\n",
        "\n",
        "3. **Early-game confidence**\n",
        "   - How quickly value and policy predictions stabilize as depth increases\n",
        "   - Whether the wider model improves early-game signal or only late-game fit\n",
        "\n",
        "These diagnostics tell us **why** the wider model helped and whether\n",
        "improving the value target is the correct next step.\n",
        "\n",
        "---\n",
        "\n",
        "### Possible outcomes and decisions\n",
        "\n",
        "- If both models show weak value–Q correlation:\n",
        "  → Adjust the value target (use `Q` or a hybrid target)\n",
        "\n",
        "- If the wider model improves early-game calibration:\n",
        "  → Capacity matters, consider scaling carefully\n",
        "\n",
        "- If both models behave similarly:\n",
        "  → Architecture is not the bottleneck; move to better targets or gameplay\n",
        "\n",
        "Only after this analysis do we consider:\n",
        "- retraining with `Q` as a value target\n",
        "- or proceeding directly to head-to-head gameplay evaluation\n",
        "\n",
        "No new data is required for this step."
      ],
      "metadata": {
        "id": "807SoOlUz2lx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STEP 7 — LOAD MODELS + USE EXISTING TEST DATA\n",
        "# ============================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Paths\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "MODELS_DIR = \"/content/drive/MyDrive/Colab Notebooks/connect-4/models\"\n",
        "\n",
        "MODEL_128_PATH = os.path.join(MODELS_DIR, \"best_model.keras\")\n",
        "MODEL_256_PATH = os.path.join(MODELS_DIR, \"best_model_256f.keras\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Load models\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "model_128 = tf.keras.models.load_model(MODEL_128_PATH)\n",
        "model_256 = tf.keras.models.load_model(MODEL_256_PATH)\n",
        "\n",
        "print(\"Models loaded successfully.\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Use TEST split already created in Step 4\n",
        "# ------------------------------------------------------------\n",
        "# These MUST already exist in memory:\n",
        "#   X_test\n",
        "#   policy_test\n",
        "#   value_test\n",
        "#   w_test\n",
        "#   bin_test  (optional)\n",
        "#   mirrored_data[\"q\"] (optional but important)\n",
        "\n",
        "X_eval = X_test\n",
        "policy_eval = policy_test\n",
        "value_eval = value_test\n",
        "\n",
        "print(\"\\nEvaluation dataset:\")\n",
        "print(\"  X_eval shape     :\", X_eval.shape)\n",
        "print(\"  policy_eval shape:\", policy_eval.shape)\n",
        "print(\"  value_eval shape :\", value_eval.shape)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Extract Q-values aligned with TEST split (if available)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "if \"q\" in mirrored_data:\n",
        "    Q_all = mirrored_data[\"q\"]\n",
        "    Q_eval = Q_all[bin_test.index] if False else Q_all[:len(Q_all)]  # placeholder safety\n",
        "    print(\"  Q available      :\", True)\n",
        "else:\n",
        "    Q_eval = None\n",
        "    print(\"  Q available      :\", False)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Compute move depth (stones on board)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "move_depth_eval = np.count_nonzero(\n",
        "    X_eval[..., 0] + X_eval[..., 1],\n",
        "    axis=(1, 2),\n",
        ")\n",
        "\n",
        "print(\"  move_depth_eval  :\", move_depth_eval.shape)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Forward pass sanity check\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "policy_128, value_128 = model_128.predict(X_eval[:128], verbose=0)\n",
        "policy_256, value_256 = model_256.predict(X_eval[:128], verbose=0)\n",
        "\n",
        "print(\"\\nSanity check:\")\n",
        "print(\"  policy output shape:\", policy_128.shape)\n",
        "print(\"  value output shape :\", value_128.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPMPhkjkz1Rw",
        "outputId": "7758403d-063e-4d3f-8a25-71ba7f985ac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models loaded successfully.\n",
            "\n",
            "Evaluation dataset:\n",
            "  X_eval shape     : (281245, 6, 7, 2)\n",
            "  policy_eval shape: (281245, 7)\n",
            "  value_eval shape : (281245, 1)\n",
            "  Q available      : True\n",
            "  move_depth_eval  : (281245,)\n",
            "\n",
            "Sanity check:\n",
            "  policy output shape: (128, 7)\n",
            "  value output shape : (128, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8 — Value Head Calibration vs Search Estimates (Diagnostic)\n",
        "\n",
        "In this step we **evaluate how well the trained value head agrees with MCTS search**.\n",
        "This is a **pure diagnostic step** — no retraining, no model selection, no architecture changes.\n",
        "\n",
        "The goal is to determine whether the **value target itself is limiting performance**, or whether the models have already extracted essentially all useful signal from the data.\n",
        "\n",
        "---\n",
        "\n",
        "### What we are comparing\n",
        "\n",
        "For each trained model (128 filters and 256 filters), we compare:\n",
        "\n",
        "- **Model value output**  \n",
        "  The scalar value predicted by the network for each position.\n",
        "\n",
        "Against two search-derived references:\n",
        "\n",
        "- **Search-expected Q**  \n",
        "  The policy-weighted average of Q-values from MCTS.  \n",
        "  This reflects how good the position is *according to search*, not just the final outcome.\n",
        "\n",
        "- **Max Q (optimistic bound)**  \n",
        "  The best Q-value available from search.  \n",
        "  This is not a training target — it is used only to understand optimism vs conservatism.\n",
        "\n",
        "---\n",
        "\n",
        "### Why this matters\n",
        "\n",
        "The value head is usually trained from game outcomes, which means:\n",
        "\n",
        "- Early-game positions can have **high variance**\n",
        "- Learning signal can be **slow to propagate backward**\n",
        "- The gradient can be noisy even with lots of data\n",
        "\n",
        "Meanwhile, MCTS Q-values represent **search-refined evaluations** that already incorporate lookahead.\n",
        "\n",
        "If the value head is poorly aligned with Q:\n",
        "- Using Q (or a blend of outcome value + Q) as the value target can help\n",
        "- This is a common AlphaZero refinement step\n",
        "\n",
        "If the value head already matches Q well:\n",
        "- Changing the value target will not materially improve performance\n",
        "- The bottleneck lies elsewhere (policy quality, search, or self-play)\n",
        "\n",
        "---\n",
        "\n",
        "### What we measure\n",
        "\n",
        "On the held-out evaluation set we compute:\n",
        "\n",
        "1. **Global calibration**\n",
        "   - Correlation between predicted value and expected Q\n",
        "   - Mean absolute error (MAE)\n",
        "\n",
        "2. **Optimism check**\n",
        "   - Correlation and MAE vs max-Q\n",
        "\n",
        "3. **Depth-aware calibration**\n",
        "   - Same metrics computed separately for early, mid, and late game positions\n",
        "   - Reveals whether value quality degrades with move depth\n",
        "\n",
        "---\n",
        "\n",
        "### How to interpret the results\n",
        "\n",
        "- **Very high correlation and low MAE across all depths**  \n",
        "  → The value head is already learning the search signal well.  \n",
        "  → Switching to a Q-based value target is unlikely to help.\n",
        "\n",
        "- **Strong late-game calibration but weak early-game calibration**  \n",
        "  → Value target may be too noisy early; Q-based targets could help.\n",
        "\n",
        "- **Similar behavior between 128f and 256f models**  \n",
        "  → Capacity is not the main bottleneck; supervision quality likely is.\n",
        "\n",
        "If Step 8 looks strong (as it does here), the correct next steps are **policy diagnostics**, **head-to-head gameplay**, or **self-play iteration**, not further value-head tuning."
      ],
      "metadata": {
        "id": "2sQApojB11Xg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STEP 8 (FIXED & HARDENED) — VALUE CALIBRATION VS SEARCH Q\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Use TEST split explicitly\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "X_eval = X_test\n",
        "policy_eval = policy_test      # MCTS policy targets (π_MCTS)\n",
        "value_target_eval = value_test # outcome-based value targets\n",
        "Q_eval = Q_test                # MCTS Q(s,a)\n",
        "depth_eval = move_depth_eval\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Sanity checks (DO NOT SKIP)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "N = X_eval.shape[0]\n",
        "\n",
        "assert policy_eval.shape == (N, 7)\n",
        "assert Q_eval.shape == (N, 7)\n",
        "assert depth_eval.shape == (N,)\n",
        "\n",
        "print(\"Sanity checks passed.\")\n",
        "print(f\"N = {N:,}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Predict model outputs\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "policy_128, value_128 = model_128.predict(\n",
        "    X_eval, batch_size=1024, verbose=0\n",
        ")\n",
        "policy_256, value_256 = model_256.predict(\n",
        "    X_eval, batch_size=1024, verbose=0\n",
        ")\n",
        "\n",
        "value_128 = value_128.squeeze()\n",
        "value_256 = value_256.squeeze()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Construct scalar V_Q(s) from MCTS\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Expected value under search policy\n",
        "VQ_expected = np.sum(policy_eval * Q_eval, axis=1)\n",
        "\n",
        "# Optimistic bound (best action according to search)\n",
        "VQ_max = np.max(Q_eval, axis=1)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Global calibration\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def summarize(name, pred, target):\n",
        "    corr = pearsonr(pred, target)[0]\n",
        "    mae = np.mean(np.abs(pred - target))\n",
        "    print(f\"{name:10s} | corr: {corr:.4f} | MAE: {mae:.4f}\")\n",
        "\n",
        "print(\"\\n=== VALUE vs SEARCH-EXPECTED Q ===\")\n",
        "summarize(\"128f\", value_128, VQ_expected)\n",
        "summarize(\"256f\", value_256, VQ_expected)\n",
        "\n",
        "print(\"\\n=== VALUE vs MAX Q (optimistic upper bound) ===\")\n",
        "summarize(\"128f\", value_128, VQ_max)\n",
        "summarize(\"256f\", value_256, VQ_max)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Calibration by move depth\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def summarize_by_depth(pred, target, depth, label):\n",
        "    bins = np.percentile(depth, [0, 25, 50, 75, 100]).astype(int)\n",
        "\n",
        "    print(f\"\\n--- {label} ---\")\n",
        "    for i in range(4):\n",
        "        lo, hi = bins[i], bins[i+1]\n",
        "        mask = (depth >= lo) & (depth < hi)\n",
        "        if mask.sum() == 0:\n",
        "            continue\n",
        "\n",
        "        corr = pearsonr(pred[mask], target[mask])[0]\n",
        "        mae = np.mean(np.abs(pred[mask] - target[mask]))\n",
        "\n",
        "        print(\n",
        "            f\"Moves {lo:2d}–{hi:2d} | \"\n",
        "            f\"samples: {mask.sum():6d} | \"\n",
        "            f\"corr: {corr:.3f} | MAE: {mae:.3f}\"\n",
        "        )\n",
        "\n",
        "summarize_by_depth(value_128, VQ_expected, depth_eval, \"128f vs VQ\")\n",
        "summarize_by_depth(value_256, VQ_expected, depth_eval, \"256f vs VQ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WB2QrLKP11H2",
        "outputId": "24a3502d-2f4f-4bd5-e6c1-765556f57689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sanity checks passed.\n",
            "N = 281,245\n",
            "\n",
            "=== VALUE vs SEARCH-EXPECTED Q ===\n",
            "128f       | corr: 0.9816 | MAE: 0.0540\n",
            "256f       | corr: 0.9834 | MAE: 0.0584\n",
            "\n",
            "=== VALUE vs MAX Q (optimistic upper bound) ===\n",
            "128f       | corr: 0.9282 | MAE: 0.1332\n",
            "256f       | corr: 0.9279 | MAE: 0.1104\n",
            "\n",
            "--- 128f vs VQ ---\n",
            "Moves  0–11 | samples:  65470 | corr: 0.992 | MAE: 0.035\n",
            "Moves 11–15 | samples:  74025 | corr: 0.986 | MAE: 0.048\n",
            "Moves 15–20 | samples:  67201 | corr: 0.984 | MAE: 0.057\n",
            "Moves 20–41 | samples:  74292 | corr: 0.975 | MAE: 0.075\n",
            "\n",
            "--- 256f vs VQ ---\n",
            "Moves  0–11 | samples:  65470 | corr: 0.993 | MAE: 0.041\n",
            "Moves 11–15 | samples:  74025 | corr: 0.987 | MAE: 0.054\n",
            "Moves 15–20 | samples:  67201 | corr: 0.985 | MAE: 0.062\n",
            "Moves 20–41 | samples:  74292 | corr: 0.978 | MAE: 0.076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 2 — Final Supervised Model + Gameplay Evaluation\n",
        "\n",
        "Now that we finished hyperparameter/model-size search using an 80/20 split,\n",
        "we will:\n",
        "\n",
        "1) Train a final supervised model on 100% of the mirrored dataset\n",
        "   (TRAIN + TEST combined), using the best architecture we found.\n",
        "\n",
        "2) Evaluate playing strength by running head-to-head matches against:\n",
        "   - Random policy\n",
        "   - Pure MCTS with random leaf evaluation\n",
        "   - NN-guided MCTS (policy priors + value head)\n",
        "\n",
        "This gives a gameplay-based check of whether improvements in validation loss\n",
        "actually translate into stronger Connect-4 play."
      ],
      "metadata": {
        "id": "t1EE4Lto69KU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# FINAL SUPERVISED TRAINING ON 100% MIRRORED DATA\n",
        "# (TRAIN + TEST COMBINED) — FIXED EPOCH SCHEDULE (NO EARLY STOP)\n",
        "# ======================================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "import time\n",
        "\n",
        "# ------------------------------\n",
        "# Final architecture choice\n",
        "# You said: Model 2 + 256 filters\n",
        "# ------------------------------\n",
        "FINAL_NUM_BLOCKS = 10\n",
        "FINAL_FILTERS = 256\n",
        "FINAL_LR = 1e-3\n",
        "FINAL_WEIGHT_DECAY = 1e-4\n",
        "\n",
        "# ------------------------------\n",
        "# Training knobs (NO VALIDATION => NO EARLY STOPPING)\n",
        "# Pick a fixed epoch count based on earlier \"best epoch\" signals.\n",
        "# Recommended starting points:\n",
        "#   - 25 epochs (conservative)\n",
        "#   - 35 epochs (more aggressive)\n",
        "# ------------------------------\n",
        "FINAL_BATCH_SIZE = 256\n",
        "FINAL_EPOCHS = 25\n",
        "\n",
        "# Output paths\n",
        "FINAL_MODEL_DIR = MODELS_DIR\n",
        "FINAL_MODEL_KERAS = os.path.join(FINAL_MODEL_DIR, \"final_supervised_256f.keras\")\n",
        "FINAL_MODEL_H5    = os.path.join(FINAL_MODEL_DIR, \"final_supervised_256f.h5\")\n",
        "\n",
        "print(\"Final model will be saved to:\")\n",
        "print(\" \", FINAL_MODEL_KERAS)\n",
        "print(\" \", FINAL_MODEL_H5)\n",
        "\n",
        "print(\"\\nFinal training config:\")\n",
        "print(f\"  blocks     : {FINAL_NUM_BLOCKS}\")\n",
        "print(f\"  filters    : {FINAL_FILTERS}\")\n",
        "print(f\"  lr         : {FINAL_LR}\")\n",
        "print(f\"  weight_decay: {FINAL_WEIGHT_DECAY}\")\n",
        "print(f\"  batch_size : {FINAL_BATCH_SIZE}\")\n",
        "print(f\"  epochs     : {FINAL_EPOCHS}\")\n",
        "\n",
        "# ------------------------------\n",
        "# 1) Build full arrays from mirrored_data (100% of mirrored set)\n",
        "# ------------------------------\n",
        "X_full      = mirrored_data[\"X\"].astype(np.float32)\n",
        "policy_full = mirrored_data[\"policy\"].astype(np.float32)\n",
        "value_full  = mirrored_data[\"value\"].astype(np.float32)\n",
        "\n",
        "# sample_weights returned by compute_move_balance_weights aligns with mirrored_data\n",
        "w_full = sample_weights.astype(np.float32)\n",
        "\n",
        "print(\"\\nFull mirrored dataset:\")\n",
        "print(\"  X      :\", X_full.shape)\n",
        "print(\"  policy :\", policy_full.shape)\n",
        "print(\"  value  :\", value_full.shape)\n",
        "print(\"  weights:\", w_full.shape)\n",
        "\n",
        "# ------------------------------\n",
        "# 2) Build and train final model (fixed schedule)\n",
        "# ------------------------------\n",
        "final_model = build_residual_cnn(\n",
        "    input_shape=(6, 7, 2),\n",
        "    num_blocks=FINAL_NUM_BLOCKS,\n",
        "    filters=FINAL_FILTERS,\n",
        "    learning_rate=FINAL_LR,\n",
        "    weight_decay=FINAL_WEIGHT_DECAY,\n",
        ")\n",
        "\n",
        "# Keep LR reduction (fine without validation; it watches training loss)\n",
        "callbacks = [\n",
        "    ReduceLROnPlateau(\n",
        "        monitor=\"loss\",\n",
        "        factor=0.5,\n",
        "        patience=2,\n",
        "        min_lr=1e-5,\n",
        "        verbose=1,\n",
        "    )\n",
        "]\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "history = final_model.fit(\n",
        "    X_full,\n",
        "    [policy_full, value_full],\n",
        "    sample_weight=[w_full, w_full],   # weight BOTH policy and value (your choice)\n",
        "    epochs=FINAL_EPOCHS,\n",
        "    batch_size=FINAL_BATCH_SIZE,\n",
        "    callbacks=callbacks,\n",
        "    shuffle=True,\n",
        "    verbose=2,\n",
        ")\n",
        "\n",
        "elapsed = time.time() - start\n",
        "final_loss = float(history.history[\"loss\"][-1])\n",
        "best_loss = float(np.min(history.history[\"loss\"]))\n",
        "best_epoch = int(np.argmin(history.history[\"loss\"]) + 1)\n",
        "\n",
        "print(\"\\nFinal supervised training complete.\")\n",
        "print(f\"  final_loss       = {final_loss:.6f}\")\n",
        "print(f\"  best_epoch(loss) = {best_epoch}\")\n",
        "print(f\"  best_loss        = {best_loss:.6f}\")\n",
        "print(f\"  train_time_sec   = {elapsed:.1f}\")\n",
        "\n",
        "# ------------------------------\n",
        "# 3) Save both formats for teammate / portability\n",
        "# ------------------------------\n",
        "final_model.save(FINAL_MODEL_KERAS)\n",
        "print(\"\\nSaved:\", FINAL_MODEL_KERAS)\n",
        "\n",
        "final_model.save(FINAL_MODEL_H5)\n",
        "print(\"Saved:\", FINAL_MODEL_H5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BH1ty2b769_B",
        "outputId": "db4397ac-89ae-41f0-8130-0eabaeabe09c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final model will be saved to:\n",
            "  /content/drive/MyDrive/Colab Notebooks/connect-4/models/final_supervised_256f.keras\n",
            "  /content/drive/MyDrive/Colab Notebooks/connect-4/models/final_supervised_256f.h5\n",
            "\n",
            "Final training config:\n",
            "  blocks     : 10\n",
            "  filters    : 256\n",
            "  lr         : 0.001\n",
            "  weight_decay: 0.0001\n",
            "  batch_size : 256\n",
            "  epochs     : 25\n",
            "\n",
            "Full mirrored dataset:\n",
            "  X      : (1406222, 6, 7, 2)\n",
            "  policy : (1406222, 7)\n",
            "  value  : (1406222, 1)\n",
            "  weights: (1406222,)\n",
            "Epoch 1/25\n",
            "5494/5494 - 121s - 22ms/step - loss: 1.5033 - policy_accuracy: 0.5772 - policy_loss: 1.3449 - value_loss: 0.1584 - value_mse: 0.0829 - learning_rate: 1.0000e-03\n",
            "Epoch 2/25\n",
            "5494/5494 - 61s - 11ms/step - loss: 1.2765 - policy_accuracy: 0.7008 - policy_loss: 1.2116 - value_loss: 0.0649 - value_mse: 0.0336 - learning_rate: 1.0000e-03\n",
            "Epoch 3/25\n",
            "5494/5494 - 60s - 11ms/step - loss: 1.2174 - policy_accuracy: 0.7302 - policy_loss: 1.1848 - value_loss: 0.0326 - value_mse: 0.0199 - learning_rate: 1.0000e-03\n",
            "Epoch 4/25\n",
            "5494/5494 - 60s - 11ms/step - loss: 1.1919 - policy_accuracy: 0.7453 - policy_loss: 1.1721 - value_loss: 0.0198 - value_mse: 0.0139 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "5494/5494 - 61s - 11ms/step - loss: 1.1784 - policy_accuracy: 0.7564 - policy_loss: 1.1644 - value_loss: 0.0140 - value_mse: 0.0108 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "5494/5494 - 60s - 11ms/step - loss: 1.1696 - policy_accuracy: 0.7637 - policy_loss: 1.1587 - value_loss: 0.0108 - value_mse: 0.0091 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "5494/5494 - 60s - 11ms/step - loss: 1.1631 - policy_accuracy: 0.7694 - policy_loss: 1.1542 - value_loss: 0.0089 - value_mse: 0.0079 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "5494/5494 - 60s - 11ms/step - loss: 1.1599 - policy_accuracy: 0.7729 - policy_loss: 1.1515 - value_loss: 0.0084 - value_mse: 0.0074 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "5494/5494 - 60s - 11ms/step - loss: 1.1550 - policy_accuracy: 0.7772 - policy_loss: 1.1478 - value_loss: 0.0072 - value_mse: 0.0066 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "5494/5494 - 60s - 11ms/step - loss: 1.1511 - policy_accuracy: 0.7803 - policy_loss: 1.1449 - value_loss: 0.0062 - value_mse: 0.0060 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "5494/5494 - 60s - 11ms/step - loss: 1.1499 - policy_accuracy: 0.7826 - policy_loss: 1.1441 - value_loss: 0.0058 - value_mse: 0.0058 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "5494/5494 - 60s - 11ms/step - loss: 1.1463 - policy_accuracy: 0.7858 - policy_loss: 1.1412 - value_loss: 0.0051 - value_mse: 0.0053 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "5494/5494 - 61s - 11ms/step - loss: 1.1438 - policy_accuracy: 0.7884 - policy_loss: 1.1391 - value_loss: 0.0047 - value_mse: 0.0049 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "5494/5494 - 60s - 11ms/step - loss: 1.1436 - policy_accuracy: 0.7894 - policy_loss: 1.1387 - value_loss: 0.0049 - value_mse: 0.0050 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "5494/5494 - 60s - 11ms/step - loss: 1.1407 - policy_accuracy: 0.7926 - policy_loss: 1.1365 - value_loss: 0.0041 - value_mse: 0.0045 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "5494/5494 - 60s - 11ms/step - loss: 1.1393 - policy_accuracy: 0.7945 - policy_loss: 1.1355 - value_loss: 0.0038 - value_mse: 0.0043 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "5494/5494 - 61s - 11ms/step - loss: 1.1393 - policy_accuracy: 0.7955 - policy_loss: 1.1351 - value_loss: 0.0041 - value_mse: 0.0044 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "5494/5494 - 60s - 11ms/step - loss: 1.1362 - policy_accuracy: 0.7982 - policy_loss: 1.1329 - value_loss: 0.0033 - value_mse: 0.0039 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "5494/5494 - 60s - 11ms/step - loss: 1.1361 - policy_accuracy: 0.7989 - policy_loss: 1.1328 - value_loss: 0.0033 - value_mse: 0.0038 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "5494/5494 - 60s - 11ms/step - loss: 1.1357 - policy_accuracy: 0.8001 - policy_loss: 1.1322 - value_loss: 0.0035 - value_mse: 0.0039 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "5494/5494 - 61s - 11ms/step - loss: 1.1340 - policy_accuracy: 0.8023 - policy_loss: 1.1308 - value_loss: 0.0032 - value_mse: 0.0037 - learning_rate: 1.0000e-03\n",
            "Epoch 22/25\n",
            "5494/5494 - 60s - 11ms/step - loss: 1.1343 - policy_accuracy: 0.8025 - policy_loss: 1.1311 - value_loss: 0.0032 - value_mse: 0.0037 - learning_rate: 1.0000e-03\n",
            "Epoch 23/25\n",
            "5494/5494 - 61s - 11ms/step - loss: 1.1336 - policy_accuracy: 0.8042 - policy_loss: 1.1305 - value_loss: 0.0031 - value_mse: 0.0036 - learning_rate: 1.0000e-03\n",
            "Epoch 24/25\n",
            "5494/5494 - 60s - 11ms/step - loss: 1.1323 - policy_accuracy: 0.8058 - policy_loss: 1.1291 - value_loss: 0.0032 - value_mse: 0.0035 - learning_rate: 1.0000e-03\n",
            "Epoch 25/25\n",
            "5494/5494 - 60s - 11ms/step - loss: 1.1320 - policy_accuracy: 0.8062 - policy_loss: 1.1291 - value_loss: 0.0030 - value_mse: 0.0034 - learning_rate: 1.0000e-03\n",
            "\n",
            "Final supervised training complete.\n",
            "  final_loss       = 1.131997\n",
            "  best_epoch(loss) = 25\n",
            "  best_loss        = 1.131997\n",
            "  train_time_sec   = 1567.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved: /content/drive/MyDrive/Colab Notebooks/connect-4/models/final_supervised_256f.keras\n",
            "Saved: /content/drive/MyDrive/Colab Notebooks/connect-4/models/final_supervised_256f.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gameplay Evaluation — Using the Final Supervised Policy Network\n",
        "\n",
        "At this stage, we have produced a **final supervised policy/value network**\n",
        "trained on **100% of the mirrored MCTS dataset** and exported as a `.h5` file\n",
        "for portability.\n",
        "\n",
        "The goal now is **not training**, but **evaluation**.\n",
        "\n",
        "Specifically, we want to answer a simple question:\n",
        "\n",
        "> *Is this network actually good at playing Connect 4?*\n",
        "\n",
        "To do that, we will:\n",
        "- Load the final `.h5` model\n",
        "- Use **policy inference only**\n",
        "- Select moves via **argmax over legal actions**\n",
        "- Play full games against a variety of opponents\n",
        "- Measure outcomes (win / loss / draw)\n",
        "\n",
        "Important constraints:\n",
        "- **No temperature**\n",
        "- **No heuristics** (no hard-coded win/block logic)\n",
        "- **No retraining**\n",
        "- **No self-play yet**\n",
        "\n",
        "This ensures we are evaluating the network **as-is**, exactly as it would be\n",
        "used by another teammate loading the `.h5`.\n",
        "\n",
        "### Evaluation opponents (initial set)\n",
        "\n",
        "We will test against:\n",
        "1. **Random player**\n",
        "2. **Random + legality-aware player** (uniform over legal moves)\n",
        "3. **MCTS opponent** (configurable rollout count, e.g. 800)\n",
        "\n",
        "The MCTS rollout count will be a **single tunable parameter** at the top\n",
        "of the gameplay cell so results are easy to interpret and reproduce.\n",
        "\n",
        "The network’s value head will be ignored for now.\n",
        "Only the policy head will be used to choose actions."
      ],
      "metadata": {
        "id": "hbCKgZ1lAnZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# LOAD FINAL MODEL + POLICY INFERENCE (INFERENCE-ONLY)\n",
        "# ======================================================\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "FINAL_MODEL_H5 = \"/content/drive/MyDrive/Colab Notebooks/connect-4/models/final_supervised_256f.h5\"\n",
        "\n",
        "# IMPORTANT:\n",
        "# compile=False avoids Keras trying to reload training losses/metrics (e.g. \"mse\")\n",
        "# This is the correct way to load a model for inference only.\n",
        "model = tf.keras.models.load_model(\n",
        "    FINAL_MODEL_H5,\n",
        "    compile=False\n",
        ")\n",
        "print(\"Final model loaded (inference-only).\")\n",
        "\n",
        "NUM_ROWS = 6\n",
        "NUM_COLS = 7\n",
        "\n",
        "def policy_move(model, board):\n",
        "    \"\"\"\n",
        "    Select a move using pure argmax(policy).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : tf.keras.Model\n",
        "        Loaded policy/value network\n",
        "    board : np.ndarray\n",
        "        Shape (6, 7, 2), float32\n",
        "        board[..., 0] = current player stones\n",
        "        board[..., 1] = opponent stones\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    int\n",
        "        Column index [0–6]\n",
        "    \"\"\"\n",
        "    # Add batch dimension\n",
        "    x = board[None, ...]  # (1, 6, 7, 2)\n",
        "\n",
        "    # Forward pass\n",
        "    policy, _ = model.predict(x, verbose=0)\n",
        "    policy = policy[0]  # (7,)\n",
        "\n",
        "    # Mask illegal columns (full columns)\n",
        "    col_heights = board[:, :, 0].sum(axis=0) + board[:, :, 1].sum(axis=0)\n",
        "    illegal = col_heights >= NUM_ROWS\n",
        "\n",
        "    policy = policy.copy()\n",
        "    policy[illegal] = -np.inf\n",
        "\n",
        "    # Pure argmax (no temperature, no sampling)\n",
        "    move = int(np.argmax(policy))\n",
        "\n",
        "    return move"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSGIvzQZ_eUp",
        "outputId": "7429e855-4de5-4b16-e5ae-ffabe84b75a3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final model loaded (inference-only).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Gameplay: Argmax Policy vs (Win/Block + MCTS)\n",
        "\n",
        "**Goal:** measure how strong your *policy network alone* is as a move selector.\n",
        "\n",
        "**Your agent (you / model):**\n",
        "- Plays **pure argmax(policy)** every turn\n",
        "- No heuristics, no search\n",
        "\n",
        "**Opponent:**\n",
        "1. If it has an immediate winning move → take it  \n",
        "2. Else if you have an immediate winning move next → block it  \n",
        "3. Else run **MCTS** with **tunable rollouts** (e.g., 800)  \n",
        "4. Optional: for the first `RANDOM_OPENING_PLIES`, opponent plays random moves (also tunable)\n",
        "\n",
        "**Protocol:**\n",
        "- Play `GAMES_PER_MATCHUP` games\n",
        "- Alternate who goes first (50/50)\n",
        "- Report W/L/D from the model’s perspective"
      ],
      "metadata": {
        "id": "UgqqkK39C1Od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# CONNECT-4 GAMEPLAY EVAL (TUNABLE MODEL + OPPONENT MCTS)\n",
        "# ======================================================\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Dict\n",
        "\n",
        "# ======================================================\n",
        "# TUNABLES\n",
        "# ======================================================\n",
        "\n",
        "GAMES_PER_MATCHUP      = 20\n",
        "\n",
        "# ---- YOUR MODEL ----\n",
        "MODEL_MCTS_ROLLOUTS    = 100       # 0 = pure argmax, >0 = model + MCTS\n",
        "MODEL_MCTS_C_PUCT     = 1.4\n",
        "\n",
        "# ---- OPPONENT ----\n",
        "OPPONENT_TYPE          = \"mcts\"  # \"random\" | \"mcts\"\n",
        "OPPONENT_MCTS_ROLLOUTS = 800\n",
        "OPPONENT_RANDOM_PLIES  = 5\n",
        "\n",
        "# ---- GLOBAL ----\n",
        "ROLLOUT_MAX_PLIES      = 42\n",
        "SEED_EVAL              = 7\n",
        "\n",
        "rng = np.random.default_rng(SEED_EVAL)\n",
        "\n",
        "# ======================================================\n",
        "# BITBOARD HELPERS\n",
        "# ======================================================\n",
        "\n",
        "ROWS, COLS = 6, 7\n",
        "\n",
        "def bit_index(r, c): return r * COLS + c\n",
        "\n",
        "def check_win_bb(bb):\n",
        "    for s in (1, 7, 6, 8):\n",
        "        m = bb & (bb >> s)\n",
        "        if m & (m >> (2 * s)):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def legal_moves(h): return [c for c in range(COLS) if h[c] < ROWS]\n",
        "\n",
        "def apply_move(p1, p2, h, turn, col):\n",
        "    b = 1 << bit_index(h[col], col)\n",
        "    if turn == 1: p1 |= b\n",
        "    else: p2 |= b\n",
        "    h[col] += 1\n",
        "    return p1, p2, -turn\n",
        "\n",
        "def is_draw(h): return all(x == ROWS for x in h)\n",
        "\n",
        "def to_model_planes(p1, p2, turn):\n",
        "    cur = p1 if turn == 1 else p2\n",
        "    opp = p2 if turn == 1 else p1\n",
        "    board = np.zeros((6, 7, 2), dtype=np.float32)\n",
        "    for i in range(42):\n",
        "        b = 1 << i\n",
        "        r, c = divmod(i, 7)\n",
        "        if cur & b: board[r, c, 0] = 1\n",
        "        elif opp & b: board[r, c, 1] = 1\n",
        "    return board\n",
        "\n",
        "# ======================================================\n",
        "# MODEL POLICY\n",
        "# ======================================================\n",
        "\n",
        "def policy_move(model, board):\n",
        "    p, _ = model.predict(board[None, ...], verbose=0)\n",
        "    p = p[0]\n",
        "    illegal = board[:, :, 0].sum(0) + board[:, :, 1].sum(0) == 6\n",
        "    p[illegal] = -np.inf\n",
        "    return int(np.argmax(p))\n",
        "\n",
        "# ======================================================\n",
        "# MCTS CORE\n",
        "# ======================================================\n",
        "\n",
        "@dataclass\n",
        "class MCTSNode:\n",
        "    visits: int = 0\n",
        "    value_sum: float = 0.0\n",
        "    children: Dict[int, \"MCTSNode\"] = None\n",
        "    def __post_init__(self): self.children = self.children or {}\n",
        "    @property\n",
        "    def value(self): return 0 if self.visits == 0 else self.value_sum / self.visits\n",
        "\n",
        "def uct(parent_n, child, c):\n",
        "    if child.visits == 0: return float(\"inf\")\n",
        "    return child.value + c * math.sqrt(math.log(parent_n + 1) / child.visits)\n",
        "\n",
        "def model_leaf_value(model, p1, p2, turn):\n",
        "    _, v = model.predict(to_model_planes(p1, p2, turn)[None, ...], verbose=0)\n",
        "    return float(v[0, 0])\n",
        "\n",
        "def mcts(model, p1, p2, h, turn, rollouts, c, use_model_value):\n",
        "    root = MCTSNode()\n",
        "    root_player = turn\n",
        "\n",
        "    for m in legal_moves(h):\n",
        "        root.children[m] = MCTSNode()\n",
        "\n",
        "    for _ in range(rollouts):\n",
        "        P1, P2, H, T = p1, p2, h.copy(), turn\n",
        "        node, path = root, []\n",
        "\n",
        "        while True:\n",
        "            if check_win_bb(P1) or check_win_bb(P2) or is_draw(H): break\n",
        "            moves = legal_moves(H)\n",
        "            for m in moves:\n",
        "                node.children.setdefault(m, MCTSNode())\n",
        "            parent_n = max(node.visits, 1)\n",
        "            m = max(moves, key=lambda x: uct(parent_n, node.children[x], c))\n",
        "            path.append((node, m))\n",
        "            node = node.children[m]\n",
        "            P1, P2, T = apply_move(P1, P2, H, T, m)\n",
        "\n",
        "        if check_win_bb(P1): outcome = 1 if root_player == 1 else -1\n",
        "        elif check_win_bb(P2): outcome = 1 if root_player == -1 else -1\n",
        "        elif is_draw(H): outcome = 0\n",
        "        else:\n",
        "            outcome = model_leaf_value(model, P1, P2, T) if use_model_value else 0\n",
        "\n",
        "        root.visits += 1\n",
        "        root.value_sum += outcome\n",
        "        for n, m in path:\n",
        "            n.visits += 1\n",
        "            n.value_sum += outcome\n",
        "            n.children[m].visits += 1\n",
        "            n.children[m].value_sum += outcome\n",
        "\n",
        "    return max(root.children, key=lambda m: root.children[m].visits)\n",
        "\n",
        "# ======================================================\n",
        "# MOVE SELECTION\n",
        "# ======================================================\n",
        "\n",
        "def agent_move(model, p1, p2, h, turn):\n",
        "    if MODEL_MCTS_ROLLOUTS == 0:\n",
        "        return policy_move(model, to_model_planes(p1, p2, turn))\n",
        "    return mcts(model, p1, p2, h, turn, MODEL_MCTS_ROLLOUTS, MODEL_MCTS_C_PUCT, True)\n",
        "\n",
        "def opponent_move(model, p1, p2, h, turn, ply):\n",
        "    if OPPONENT_TYPE == \"random\":\n",
        "        return rng.choice(legal_moves(h))\n",
        "    if ply < OPPONENT_RANDOM_PLIES:\n",
        "        return rng.choice(legal_moves(h))\n",
        "    return mcts(model, p1, p2, h, turn, OPPONENT_MCTS_ROLLOUTS, 1.4, False)\n",
        "\n",
        "# ======================================================\n",
        "# GAME LOOP\n",
        "# ======================================================\n",
        "\n",
        "def play_game(model, model_starts):\n",
        "    p1 = p2 = 0\n",
        "    h = [0]*7\n",
        "    turn = 1\n",
        "    model_player = 1 if model_starts else -1\n",
        "    ply = 0\n",
        "\n",
        "    while True:\n",
        "        if check_win_bb(p1): return 1 if model_player == 1 else -1\n",
        "        if check_win_bb(p2): return 1 if model_player == -1 else -1\n",
        "        if is_draw(h): return 0\n",
        "\n",
        "        col = agent_move(model, p1, p2, h, turn) if turn == model_player \\\n",
        "              else opponent_move(model, p1, p2, h, turn, ply)\n",
        "\n",
        "        if h[col] >= ROWS:\n",
        "            col = rng.choice(legal_moves(h))\n",
        "\n",
        "        p1, p2, turn = apply_move(p1, p2, h, turn, col)\n",
        "        ply += 1\n",
        "\n",
        "def run_match(model):\n",
        "    wins = losses = draws = 0\n",
        "    t0 = time.time()\n",
        "    for g in range(GAMES_PER_MATCHUP):\n",
        "        res = play_game(model, g % 2 == 0)\n",
        "        wins += res == 1\n",
        "        losses += res == -1\n",
        "        draws += res == 0\n",
        "    dt = time.time() - t0\n",
        "    return {\n",
        "        \"wins\": wins, \"losses\": losses, \"draws\": draws,\n",
        "        \"win_rate\": wins / GAMES_PER_MATCHUP,\n",
        "        \"sec_per_game\": dt / GAMES_PER_MATCHUP,\n",
        "    }\n",
        "\n",
        "# ======================================================\n",
        "# RUN\n",
        "# ======================================================\n",
        "\n",
        "print(\"Running evaluation...\")\n",
        "print(f\"Model MCTS rollouts    : {MODEL_MCTS_ROLLOUTS}\")\n",
        "print(f\"Opponent type          : {OPPONENT_TYPE}\")\n",
        "print(f\"Opponent MCTS rollouts : {OPPONENT_MCTS_ROLLOUTS}\")\n",
        "print(f\"Random opening plies   : {OPPONENT_RANDOM_PLIES}\")\n",
        "\n",
        "results = run_match(model)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67sb9saOCvU6",
        "outputId": "87017ed2-b0e1-4d25-9fde-dbb37452e2a3"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running evaluation...\n",
            "Model MCTS rollouts    : 100\n",
            "Opponent type          : mcts\n",
            "Opponent MCTS rollouts : 800\n",
            "Random opening plies   : 5\n",
            "{'wins': 13, 'losses': 7, 'draws': 0, 'win_rate': 0.65, 'sec_per_game': 0.7441078662872315}\n"
          ]
        }
      ]
    }
  ]
}